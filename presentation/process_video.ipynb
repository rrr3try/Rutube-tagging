{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "3e978593-6362-4169-b6a3-038c80dcfd95",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true,
        "id": "3e978593-6362-4169-b6a3-038c80dcfd95"
      },
      "source": [
        "## Импорты"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c7bdd6d8-1316-4d74-abd9-207f918a73e5",
      "metadata": {
        "id": "c7bdd6d8-1316-4d74-abd9-207f918a73e5"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchaudio\n",
        "\n",
        "import yt_dlp\n",
        "import subprocess\n",
        "\n",
        "import os\n",
        "import pickle\n",
        "import gc\n",
        "import json\n",
        "import locale\n",
        "import re\n",
        "import tqdm.notebook as tqdm\n",
        "from urllib.parse import urlparse\n",
        "import requests\n",
        "import math\n",
        "\n",
        "import av\n",
        "from huggingface_hub import hf_hub_download\n",
        "\n",
        "from transformers import VivitImageProcessor, VivitModel\n",
        "from transformers import AutoImageProcessor, VideoMAEModel\n",
        "from transformers import TimesformerConfig, TimesformerModel\n",
        "from transformers import XCLIPProcessor, XCLIPModel\n",
        "from transformers import AutoModelForSpeechSeq2Seq, AutoProcessor\n",
        "from transformers import Wav2Vec2ForCTC, Wav2Vec2Processor\n",
        "from transformers import BitsAndBytesConfig, LlavaNextVideoForConditionalGeneration, LlavaNextVideoProcessor\n",
        "from transformers import AutoModelForSpeechSeq2Seq, AutoProcessor, pipeline\n",
        "\n",
        "import librosa\n",
        "from moviepy.editor import VideoFileClip\n",
        "import ast\n",
        "import openunmix\n",
        "\n",
        "from minio import Minio\n",
        "from minio.error import S3Error\n",
        "\n",
        "import hydra\n",
        "import soundfile as sf\n",
        "from omegaconf import OmegaConf\n",
        "\n",
        "import logging\n",
        "from typing import List\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.cross_decomposition import PLSRegression\n",
        "from sklearn.preprocessing import StandardScaler, LabelBinarizer\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "\n",
        "locale.getpreferredencoding = lambda: \"UTF-8\"\n",
        "\n",
        "logging.basicConfig(\n",
        "    filename='vivit_inference.log',\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a1c057b8-4ae5-4dcb-87b9-6395696b8f24",
      "metadata": {
        "id": "a1c057b8-4ae5-4dcb-87b9-6395696b8f24"
      },
      "source": [
        "Константы"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "79f4074a-a223-4007-aafd-bd3e3ce2a4cb",
      "metadata": {
        "id": "79f4074a-a223-4007-aafd-bd3e3ce2a4cb"
      },
      "outputs": [],
      "source": [
        "CATEGORIES_ENG = \"\"\"Auto-moto,\n",
        "Anime,\n",
        "Audiobooks,\n",
        "Business,\n",
        "Video games,\n",
        "Interview,\n",
        "Art,\n",
        "Movie,\n",
        "Beauty,\n",
        "Cooking,\n",
        "Life Hacks,\n",
        "Music,\n",
        "Cartoons,\n",
        "News,\n",
        "Training,\n",
        "Hunting and fishing,\n",
        "Politics,\n",
        "Psychology,\n",
        "Journeys,\n",
        "Serials,\n",
        "Sport,\n",
        "Humor,\n",
        "Lifestyle,\n",
        "Realty,\n",
        "Health,\n",
        "Nature,\n",
        "Design,\n",
        "Machinery and equipment,\n",
        "Business and entrepreneurship,\n",
        "Culture,\n",
        "Religion,\n",
        "Construction and renovation,\n",
        "Garden and vegetable garden,\n",
        "Food,\n",
        "Entertainment,\n",
        "Esotericism,\n",
        "The science,\n",
        "Audio,\n",
        "Technology and the Internet,\n",
        "TV shows,\n",
        "For children,\n",
        "Hobby,\n",
        "Various,\n",
        "Animals,\n",
        "News and media,\n",
        "Films,\n",
        "Bloggers,\n",
        "\"\"\"\n",
        "\n",
        "CATEGORIES_RUS = \"\"\"\n",
        "Авто-мото\n",
        "Аниме\n",
        "Аудиокниги\n",
        "Бизнес\n",
        "Видеоигры\n",
        "Интервью\n",
        "Искусство\n",
        "Кино\n",
        "Красота\n",
        "Кулинария\n",
        "Лайфхаки\n",
        "Музыка\n",
        "Мультфильмы\n",
        "Новости\n",
        "Обучение\n",
        "Охота_и_рыбалка\n",
        "Политика\n",
        "Психология\n",
        "Путешествия\n",
        "Сериалы\n",
        "Спорт\n",
        "Юмор\n",
        "Лайфстайл\n",
        "Недвижимость\n",
        "Здоровье\n",
        "Природа\n",
        "Дизайн\n",
        "Техника_и_оборудование\n",
        "Бизнес_и_предпринимательство\n",
        "Культура\n",
        "Религия\n",
        "Строительство_и_ремонт\n",
        "Сад_и_огород\n",
        "Еда\n",
        "Развлечения\n",
        "Эзотерика\n",
        "Наука\n",
        "Аудио\n",
        "Технологии_и_интернет\n",
        "Телепередачи\n",
        "Детям\n",
        "Хобби\n",
        "Разное\n",
        "Животные\n",
        "Новости_и_СМИ\n",
        "Фильмы\n",
        "Блогеры\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "RUS_TEXT_PROMPT = [\n",
        "    f\"Видео принадлежит категории '{x}'\" for x in CATEGORIES_RUS.split() if x.strip() != ''\n",
        "]\n",
        "ENG_TEXT_PROMPT = [\n",
        "    f\"Video belongs to category '{x}'\" for x in CATEGORIES_ENG.split(',\\n') if x.strip() != ''\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6323d233-6664-4d82-9e01-3656e9b596ad",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true,
        "id": "6323d233-6664-4d82-9e01-3656e9b596ad"
      },
      "source": [
        "## Download video"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ad269fc7-426e-40bc-830c-fbc65681c0e2",
      "metadata": {
        "id": "ad269fc7-426e-40bc-830c-fbc65681c0e2"
      },
      "outputs": [],
      "source": [
        "client = Minio(\n",
        "    endpoint=\"storage.yandexcloud.net\",\n",
        "    access_key=\"YCAJESQqZUja9X-F1glArEPSY\",\n",
        "    secret_key=\"YCP6M_QUdKUF1XBlgz_hOWAlTkcMbnEUyLG5hsQv\",\n",
        ")\n",
        "\n",
        "BUCKET_NAME = \"rutube-tagging\"\n",
        "\n",
        "\n",
        "def list_files_in_bucket(bucket_name=BUCKET_NAME):\n",
        "    \"\"\"\n",
        "    List all files in the S3 bucket.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        objects = client.list_objects(bucket_name)\n",
        "        for obj in objects:\n",
        "            print(obj.object_name)\n",
        "    except S3Error as e:\n",
        "        print(f\"Error listing objects in bucket: {e}\")\n",
        "\n",
        "\n",
        "def download_video_from_s3_url(url, output_directory='.'):\n",
        "    \"\"\"\n",
        "    Downloads a video from an S3-compatible URL using the MinIO client.\n",
        "\n",
        "    :param url: The S3 URL of the video to download.\n",
        "    :param output_directory: The directory where the video will be saved.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Parse the URL to extract bucket and object name\n",
        "        parsed_url = urlparse(url)\n",
        "        path_parts = parsed_url.path.lstrip('/').split('/', 1)\n",
        "\n",
        "        if len(path_parts) != 2:\n",
        "            raise ValueError(\"URL does not contain both bucket and object name.\")\n",
        "\n",
        "        bucket_name, object_name = path_parts\n",
        "\n",
        "        # Ensure the output directory exists\n",
        "        os.makedirs(output_directory, exist_ok=True)\n",
        "\n",
        "        # Define the local file path\n",
        "        filename = os.path.basename(object_name)\n",
        "        local_file_path = os.path.join(output_directory, filename)\n",
        "\n",
        "        print(f\"Starting download of {url} using MinIO client...\")\n",
        "\n",
        "        # Download the object\n",
        "        client.fget_object(\n",
        "            bucket_name, object_name, local_file_path\n",
        "        )\n",
        "\n",
        "        print(f\"Download completed. Saved to {local_file_path}\")\n",
        "\n",
        "    except S3Error as e:\n",
        "        print(f\"S3 error occurred: {e}\")\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {e}\")\n",
        "\n",
        "\n",
        "def download_video_from_url(url, output_directory='.'):\n",
        "    \"\"\"\n",
        "    Downloads a video from the given URL to the specified output directory.\n",
        "\n",
        "    :param url: The URL of the video to download.\n",
        "    :param output_directory: The directory where the video will be saved.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Ensure the output directory exists\n",
        "        os.makedirs(output_directory, exist_ok=True)\n",
        "\n",
        "        # Extract the filename from the URL\n",
        "        filename = os.path.basename(url)\n",
        "        local_file_path = os.path.join(output_directory, filename)\n",
        "\n",
        "        print(f\"Starting download of {url}...\")\n",
        "\n",
        "        # Stream the download to handle large files\n",
        "        with requests.get(url, stream=True) as response:\n",
        "            response.raise_for_status()  # Check for HTTP errors\n",
        "            with open(local_file_path, 'wb') as f:\n",
        "                for chunk in tqdm.tqdm(response.iter_content(chunk_size=8192)):\n",
        "                    if chunk:  # Filter out keep-alive chunks\n",
        "                        f.write(chunk)\n",
        "\n",
        "        print(f\"Download completed. Saved to {local_file_path}\")\n",
        "\n",
        "    except requests.exceptions.HTTPError as http_err:\n",
        "        print(f\"HTTP error occurred while downloading {url}: {http_err}\")\n",
        "    except Exception as err:\n",
        "        print(f\"An error occurred while downloading {url}: {err}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d3621451-7d25-4808-b085-e54ac8e4fba9",
      "metadata": {
        "id": "d3621451-7d25-4808-b085-e54ac8e4fba9"
      },
      "outputs": [],
      "source": [
        "np.random.seed(0)\n",
        "torch.manual_seed(0)\n",
        "\n",
        "\n",
        "def get_model_params_count(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "\n",
        "def sample_frame_indices(desired_frames: int, frame_sample_rate: int, total_frames: int) -> np.ndarray:\n",
        "    indices = np.arange(0, frame_sample_rate*desired_frames, frame_sample_rate)\n",
        "    if indices[-1] >= total_frames:\n",
        "        indices = np.clip(indices, 0, total_frames-1)\n",
        "\n",
        "    return indices\n",
        "\n",
        "\n",
        "def read_video_pyav(container: av.container.input.InputContainer, indices: List[int]) -> np.ndarray:\n",
        "    from collections import Counter\n",
        "\n",
        "    frames = []\n",
        "    container.seek(0)\n",
        "    indices_dct = Counter(indices)\n",
        "\n",
        "    max_idx = indices[-1]\n",
        "\n",
        "    for i, frame in enumerate(container.decode(video=0)):\n",
        "        if i > max_idx:\n",
        "            break\n",
        "\n",
        "        if i in indices_dct:\n",
        "            while indices_dct[i] > 0:\n",
        "                frames.append(frame)\n",
        "                indices_dct[i] -= 1\n",
        "\n",
        "    return np.stack([x.to_ndarray(format=\"rgb24\") for x in frames])\n",
        "\n",
        "\n",
        "def process_video(\n",
        "    video_path: str,\n",
        "    image_processor,\n",
        "    model,\n",
        "    device: str = 'cuda',\n",
        "    desired_frames: int = 96,\n",
        "    window_size: int = 32,\n",
        "    xclip_text_prompt: list = RUS_TEXT_PROMPT\n",
        ") -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Processes a Rutube video: extracts the first two minutes, samples 96 frames,\n",
        "    and returns the ViViT model's output.\n",
        "\n",
        "    :param video_path: Path to the video file.\n",
        "    :param window_size: Size of each window for ViViT.\n",
        "    :return: Model's output tensor.\n",
        "    \"\"\"\n",
        "    assert desired_frames % window_size == 0\n",
        "\n",
        "    try:\n",
        "        # Open video with PyAV\n",
        "        container = av.open(video_path)\n",
        "        video_stream = container.streams.video[0]\n",
        "\n",
        "        # Get total frames\n",
        "        total_frames = video_stream.frames\n",
        "        if total_frames == 0:\n",
        "            duration = float(video_stream.duration * video_stream.time_base)\n",
        "            fps = float(video_stream.average_rate)\n",
        "            total_frames = int(duration * fps)\n",
        "        else:\n",
        "            fps = float(video_stream.average_rate)\n",
        "            duration = float(video_stream.duration * video_stream.time_base)\n",
        "\n",
        "        # Determine target duration (first minute or actual duration)\n",
        "        target_duration = min(60, duration)  # seconds\n",
        "\n",
        "        # Calculate frame_sample_rate and frame indices, read sampled frames\n",
        "        frame_sample_rate = max(1, int(math.floor((fps * target_duration) / desired_frames)))\n",
        "        indices = sample_frame_indices(desired_frames, frame_sample_rate, total_frames)\n",
        "        video_frames = read_video_pyav(container, indices)\n",
        "\n",
        "        if len(video_frames) == 0:\n",
        "            raise ValueError(\"No frames were extracted from the video.\")\n",
        "\n",
        "        # Process inputs with image_processor\n",
        "        if isinstance(model, XCLIPModel):\n",
        "            inputs = image_processor(\n",
        "                text=xclip_text_prompt,\n",
        "                videos=list(video_frames),\n",
        "                return_tensors=\"pt\",\n",
        "                padding=True,\n",
        "                max_length=1024\n",
        "            )\n",
        "        else:\n",
        "            inputs = image_processor(\n",
        "                list(video_frames),\n",
        "                return_tensors=\"pt\",\n",
        "                padding=True\n",
        "            )\n",
        "        inputs['pixel_values'] = torch.concat(torch.split(inputs['pixel_values'], window_size, dim=1))\n",
        "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "\n",
        "        # Forward pass\n",
        "        with torch.no_grad():\n",
        "            outputs = model(**inputs)\n",
        "\n",
        "        # Extract desired output\n",
        "        if hasattr(outputs, 'pooler_output'):\n",
        "            output = outputs.pooler_output.mean(dim=0).cpu()\n",
        "            return output\n",
        "        elif hasattr(outputs, 'last_hidden_state'):\n",
        "            output = outputs.last_hidden_state.mean(dim=[0, 1]).cpu()\n",
        "            return output\n",
        "        elif isinstance(model, XCLIPModel):\n",
        "            probs = outputs.logits_per_video.cpu().squeeze().softmax(dim=-1)\n",
        "            video_emb = outputs.video_embeds.cpu().squeeze()\n",
        "            return video_emb, probs\n",
        "        else:\n",
        "            raise AttributeError(\"Model output does not contain 'pooler_output' or 'last_hidden_state'.\")\n",
        "\n",
        "        return output\n",
        "\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Unexpected error: {e}\")\n",
        "        print(\"An unexpected error occurred:\")\n",
        "        print(e)\n",
        "\n",
        "\n",
        "def instantiate_model(model_name: str, device: str = 'cuda'):\n",
        "    if model_name == 'vivit':\n",
        "        model = VivitModel.from_pretrained(\"models/vivit_model\")\n",
        "        image_processor = VivitImageProcessor.from_pretrained(\"models/vivit_image_processor\")\n",
        "        window_size = 32\n",
        "    elif model_name == 'timesformer':\n",
        "        model = TimesformerModel.from_pretrained(\"models/timesformer_model\")\n",
        "        image_processor = AutoImageProcessor.from_pretrained(\"models/timesformer_image_processor\")\n",
        "        window_size = 8\n",
        "    elif model_name == 'videomae':\n",
        "        model = VideoMAEModel.from_pretrained(\"models/videomae_model\")\n",
        "        image_processor = AutoImageProcessor.from_pretrained(\"models/videomae_image_processor\")\n",
        "        window_size = 16\n",
        "    elif model_name == 'x-clip':\n",
        "        model = XCLIPModel.from_pretrained(\"models/xclip_model\")\n",
        "        image_processor = XCLIPProcessor.from_pretrained(\"models/xclip_image_processor\")\n",
        "        window_size = 32\n",
        "    else:\n",
        "        raise ValueError(f\"{model_name} is not supported, should be in ('vivit', 'timesformer', 'videomae', 'x-clip')\")\n",
        "\n",
        "\n",
        "    model = model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    return model, image_processor, window_size"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2bda6f77-ac5b-4db2-b25c-3d83842f0858",
      "metadata": {
        "id": "2bda6f77-ac5b-4db2-b25c-3d83842f0858"
      },
      "source": [
        "## Calculate embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "db5bb02d-963e-494b-968e-3ef4d0517d3f",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true,
        "id": "db5bb02d-963e-494b-968e-3ef4d0517d3f"
      },
      "source": [
        "### Save storage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "48dff4c2-07d8-4692-adad-48625d224d9a",
      "metadata": {
        "id": "48dff4c2-07d8-4692-adad-48625d224d9a"
      },
      "outputs": [],
      "source": [
        "class EmbeddingStorage:\n",
        "    def __init__(self, labels=None, filenames=None, embeddings=None):\n",
        "        \"\"\"\n",
        "        Initialize the EmbeddingStorage class.\n",
        "\n",
        "        Args:\n",
        "            labels (list or np.ndarray): An array of labels for the embeddings.\n",
        "            filenames (list or np.ndarray): An array of filenames associated with the embeddings.\n",
        "            embeddings (np.ndarray): A NumPy array containing all embeddings.\n",
        "        \"\"\"\n",
        "        self.labels = np.array(labels) if labels is not None else np.array([])\n",
        "        self.filenames = np.array(filenames) if filenames is not None else np.array([])\n",
        "        self.embeddings = np.array(embeddings) if embeddings is not None else np.empty((0,))\n",
        "\n",
        "    def add_embedding(self, label, filename, embedding):\n",
        "        \"\"\"\n",
        "        Add a new embedding, along with its label and filename.\n",
        "\n",
        "        Args:\n",
        "            label (int or str): The label of the embedding.\n",
        "            filename (str): The filename associated with the embedding.\n",
        "            embedding (np.ndarray or torch.Tensor): The embedding to add (can be a NumPy array or Tensor).\n",
        "        \"\"\"\n",
        "        if isinstance(embedding, np.ndarray):\n",
        "            emb_array = embedding\n",
        "        else:\n",
        "            # Convert torch.Tensor to NumPy\n",
        "            emb_array = embedding.cpu().numpy()\n",
        "\n",
        "        # Append the new data\n",
        "        self.labels = np.append(self.labels, label)\n",
        "        self.filenames = np.append(self.filenames, filename)\n",
        "\n",
        "        if self.embeddings.size == 0:\n",
        "            self.embeddings = emb_array.reshape(1, -1)\n",
        "        else:\n",
        "            self.embeddings = np.vstack([self.embeddings, emb_array])\n",
        "\n",
        "    def save_to_file(self, file_path):\n",
        "        \"\"\"\n",
        "        Save the embeddings, labels, and filenames to a file (as .npz).\n",
        "\n",
        "        Args:\n",
        "            file_path (str): The path to save the .npz file.\n",
        "        \"\"\"\n",
        "        np.savez(file_path, labels=self.labels, filenames=self.filenames, embeddings=self.embeddings)\n",
        "\n",
        "    @classmethod\n",
        "    def load_from_file(cls, file_path):\n",
        "        \"\"\"\n",
        "        Load embeddings, labels, and filenames from a saved .npz file.\n",
        "\n",
        "        Args:\n",
        "            file_path (str): The path to the .npz file to load.\n",
        "\n",
        "        Returns:\n",
        "            EmbeddingStorage: An instance of EmbeddingStorage with loaded data.\n",
        "        \"\"\"\n",
        "        data = np.load(file_path)\n",
        "        return cls(labels=data['labels'], filenames=data['filenames'], embeddings=data['embeddings'])\n",
        "\n",
        "    def get_embedding_by_filename(self, filename):\n",
        "        \"\"\"\n",
        "        Retrieve an embedding by its associated filename.\n",
        "\n",
        "        Args:\n",
        "            filename (str): The filename to search for.\n",
        "\n",
        "        Returns:\n",
        "            np.ndarray: The corresponding embedding or None if not found.\n",
        "        \"\"\"\n",
        "        if filename in self.filenames:\n",
        "            idx = np.where(self.filenames == filename)[0][0]\n",
        "            return self.embeddings[idx]\n",
        "        else:\n",
        "            return None\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"\n",
        "        Return the number of embeddings stored.\n",
        "        \"\"\"\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"\n",
        "        Retrieve the label, filename, and embedding by index.\n",
        "\n",
        "        Args:\n",
        "            idx (int): The index of the embedding to retrieve.\n",
        "\n",
        "        Returns:\n",
        "            tuple: A tuple containing (label, filename, embedding).\n",
        "        \"\"\"\n",
        "        if idx >= len(self.labels):\n",
        "            raise IndexError(\"Index out of range\")\n",
        "        return self.labels[idx], self.filenames[idx], self.embeddings[idx]\n",
        "\n",
        "    def __repr__(self):\n",
        "        return f\"EmbeddingStorage(labels={len(self.labels)}, filenames={len(self.filenames)}, embeddings_shape={self.embeddings.shape})\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7f3dd282-3ab2-4960-93d8-e97931679d1f",
      "metadata": {
        "id": "7f3dd282-3ab2-4960-93d8-e97931679d1f"
      },
      "source": [
        "### Video"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0854b556-7c6e-4efe-be40-28fb2ea7483d",
      "metadata": {
        "id": "0854b556-7c6e-4efe-be40-28fb2ea7483d"
      },
      "outputs": [],
      "source": [
        "if 'mapping_df' in locals():\n",
        "    del mapping_df\n",
        "\n",
        "mapping_df = pd.read_csv('train_data_categories.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8837ca96-65ef-42b4-85e8-033cc43949c4",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true,
        "id": "8837ca96-65ef-42b4-85e8-033cc43949c4"
      },
      "source": [
        "#### Upload rest of the videos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1cdcad1e-3e24-4ff4-90b1-16fc2937daef",
      "metadata": {
        "id": "1cdcad1e-3e24-4ff4-90b1-16fc2937daef"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "\n",
        "data_dir = './data/'\n",
        "directory = os.fsencode(data_dir)\n",
        "\n",
        "for file in os.listdir(directory):\n",
        "    filename = os.fsdecode(file)\n",
        "    fullpath = os.path.join(data_dir, filename)\n",
        "\n",
        "    if os.path.isdir(fullpath):\n",
        "        if len(os.listdir(fullpath)) <= 6:\n",
        "            print(filename, len(os.listdir(fullpath)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cdc1a26f-71c0-4823-9663-308e4a073f6c",
      "metadata": {
        "id": "cdc1a26f-71c0-4823-9663-308e4a073f6c"
      },
      "outputs": [],
      "source": [
        "def extract_s3_id(s3_url: str) -> str:\n",
        "    if s3_url is np.nan:\n",
        "        return ''\n",
        "\n",
        "    last_backslash_idx = len(s3_url) - s3_url[::-1].find('/')\n",
        "    return s3_url[last_backslash_idx:]\n",
        "\n",
        "\n",
        "def remove_slashes(s: str) -> str:\n",
        "    if not isinstance(s, str):\n",
        "        return ''\n",
        "\n",
        "    chars_to_remove = [\n",
        "        '/', '\\\\',            # Standard slashes\n",
        "        \"'\", '\"',             # Standard quotes\n",
        "        '«', '»',             # Guillemets\n",
        "        '“', '”',             # Curly double quotes\n",
        "        '‘', '’',             # Curly single quotes\n",
        "        '＂',                 # Fullwidth double quote\n",
        "        '⧸',                   # Slash-like character\n",
        "        '｜', '│',            # Vertical bar-like characters\n",
        "        # Add any other auxiliary characters as needed\n",
        "    ]\n",
        "\n",
        "    translation_table = str.maketrans('', '', ''.join(chars_to_remove))\n",
        "    cleaned_string = s.translate(translation_table)\n",
        "    cleaned_string = re.sub(r'\\s+', ' ', cleaned_string).strip()\n",
        "\n",
        "    return cleaned_string\n",
        "\n",
        "\n",
        "CATEGORIES_TO_UPLOAD = [\n",
        "    \"48.0_films\",\n",
        "    \"19.0_serials\",\n",
        "    \"7.0_movie\",\n",
        "    \"29.0_design\",\n",
        "    \"46.0_animals\",\n",
        "    \"45.0_various\",\n",
        "    \"5.0_interview\",\n",
        "    \"42.0_tv shows\",\n",
        "    \"35.0_garden and vegetable garden\",\n",
        "]\n",
        "\n",
        "mapping_df = pd.read_csv('scraped_dataset.csv')\n",
        "mapping_df['s3_name'] = mapping_df['s3_url'].apply(extract_s3_id)\n",
        "mapping_df['video_name'] = mapping_df['parsed_category'].apply(lambda tuple_str: ast.literal_eval(tuple_str)[0])\n",
        "mapping_df.dropna(subset=['video_name'], inplace=True)\n",
        "\n",
        "data_dir = './data/'\n",
        "\n",
        "for cat in tqdm.tqdm(CATEGORIES_TO_UPLOAD):\n",
        "    ctg_id = extract_category_id(cat)\n",
        "    df_slice = mapping_df.query(f\"manual_category_id == {ctg_id}\")\n",
        "\n",
        "    subdir_name = os.path.join(data_dir, cat)\n",
        "    downloaded_videos = list(map(remove_slashes, os.listdir(subdir_name)))\n",
        "\n",
        "    for id, row in df_slice.iterrows():\n",
        "        video_name = remove_slashes(row['video_name'])\n",
        "\n",
        "        if video_name + '.mp4' not in downloaded_videos:\n",
        "            download_video_from_url(row['s3_url'], subdir_name)\n",
        "            # print(row['url'], subdir_name)\n",
        "            # download_rutube_video_first_two_minutes(row['url'], subdir_name)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7ff0cf0e-9e2d-4fad-92f2-f65a024f41b8",
      "metadata": {
        "id": "7ff0cf0e-9e2d-4fad-92f2-f65a024f41b8"
      },
      "source": [
        "#### Inference ViViT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "49a4505f-dba1-430b-bdc3-2c760fbe8806",
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "dc96b4f6c2724834a026c7337119678d"
          ]
        },
        "id": "49a4505f-dba1-430b-bdc3-2c760fbe8806",
        "outputId": "ad3ecee9-5169-41a0-a29f-501f3f6f0dd3"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "dc96b4f6c2724834a026c7337119678d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/48 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "def extract_category_id(ctgry: str) -> int:\n",
        "    return int(float(ctgry[:3]))\n",
        "\n",
        "\n",
        "DATA_DIR = './data'\n",
        "model_name = 'vivit'\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "model, processor, window_size = instantiate_model(model_name, device)\n",
        "embeddings_vivit = []\n",
        "\n",
        "for data_subdir in tqdm.tqdm(os.listdir(DATA_DIR)):\n",
        "    fullpath = os.path.join(DATA_DIR, data_subdir)\n",
        "\n",
        "    if not os.path.isdir(fullpath):\n",
        "        continue\n",
        "\n",
        "    video_files = [f for f in os.listdir(fullpath) if f.lower().endswith('.mp4')]\n",
        "    logging.info(f\"Found {len(video_files)} video(s) in directory '{data_subdir}'.\")\n",
        "\n",
        "    for video_file in video_files:\n",
        "        file_path = os.path.join(fullpath, video_file)\n",
        "        output = process_video(file_path, processor, model, device, desired_frames=96, window_size=window_size)\n",
        "\n",
        "        if output is not None:\n",
        "            logging.info(f\"Successfully processed video: {video_file}\")\n",
        "            embeddings_vivit.append((video_file, extract_category_id(data_subdir), output.numpy()))\n",
        "        else:\n",
        "            logging.warning(f\"Failed to process video: {video_file}\")\n",
        "\n",
        "vivit_df = EmbeddingStorage(\n",
        "    filenames=list(map(lambda x: x[0], embeddings_vivit)),\n",
        "    labels=list(map(lambda x: x[1], embeddings_vivit)),\n",
        "    embeddings=list(map(lambda x: x[2], embeddings_vivit)),\n",
        ")\n",
        "vivit_df.save_to_file('data/embeddings/vivit.npz')\n",
        "\n",
        "# vivit_df = pd.DataFrame(data=embeddings_vivit, columns=['videoname', 'label', 'emb'])\n",
        "# vivit_df.to_csv('data/embeddings/vivit.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "44c8fe54-71dd-4e50-aca5-d783719cfd11",
      "metadata": {
        "id": "44c8fe54-71dd-4e50-aca5-d783719cfd11"
      },
      "source": [
        "#### Inference X-CLIP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9b494556-da83-41a3-939d-dfdbff43f0b3",
      "metadata": {
        "id": "9b494556-da83-41a3-939d-dfdbff43f0b3"
      },
      "outputs": [],
      "source": [
        "del model, processor\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "52801d56-b70c-4da5-a44b-f2a7e9e2100c",
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "b845665b679343ddaa65d0a26b89b4d8"
          ]
        },
        "id": "52801d56-b70c-4da5-a44b-f2a7e9e2100c",
        "outputId": "bed6e51f-08e4-44e0-a5d5-ec64e88eeedc"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b845665b679343ddaa65d0a26b89b4d8",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/48 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "def extract_category_id(ctgry: str) -> int:\n",
        "    return int(float(ctgry[:3]))\n",
        "\n",
        "\n",
        "DATA_DIR = './data'\n",
        "model_name = 'x-clip'\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "model, processor, window_size = instantiate_model(model_name, device)\n",
        "embeddings_xclip = []\n",
        "\n",
        "for data_subdir in tqdm.tqdm(os.listdir(DATA_DIR)):\n",
        "    fullpath = os.path.join(DATA_DIR, data_subdir)\n",
        "\n",
        "    if not os.path.isdir(fullpath):\n",
        "        continue\n",
        "\n",
        "    video_files = [f for f in os.listdir(fullpath) if f.lower().endswith('.mp4')]\n",
        "    logging.info(f\"Found {len(video_files)} video(s) in directory '{data_subdir}'.\")\n",
        "\n",
        "    for video_file in video_files:\n",
        "        file_path = os.path.join(fullpath, video_file)\n",
        "        output = process_video(file_path, processor, model, device, desired_frames=96, window_size=window_size)\n",
        "\n",
        "        if output is not None:\n",
        "            logging.info(f\"Successfully processed video: {video_file}\")\n",
        "            embeddings_xclip.append((video_file, extract_category_id(data_subdir), output[0].numpy(), output[1].numpy()))\n",
        "        else:\n",
        "            logging.warning(f\"Failed to process video: {video_file}\")\n",
        "\n",
        "xclip_emb_df = EmbeddingStorage(\n",
        "    filenames=list(map(lambda x: x[0], embeddings_xclip)),\n",
        "    labels=list(map(lambda x: x[1], embeddings_xclip)),\n",
        "    embeddings=list(map(lambda x: x[2], embeddings_xclip)),\n",
        ")\n",
        "xclip_emb_df.save_to_file('data/embeddings/xclip_emb.npz')\n",
        "\n",
        "xclip_prob_df = EmbeddingStorage(\n",
        "    filenames=list(map(lambda x: x[0], embeddings_xclip)),\n",
        "    labels=list(map(lambda x: x[1], embeddings_xclip)),\n",
        "    embeddings=list(map(lambda x: x[3], embeddings_xclip)),\n",
        ")\n",
        "xclip_prob_df.save_to_file('data/embeddings/xclip_prob.npz')\n",
        "\n",
        "# xclip_df = pd.DataFrame(data=embeddings_xclip, columns=['videoname', 'label', 'emb', 'prob'])\n",
        "# xclip_df.to_csv('data/embeddings/xclip.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fdd11d65-56d8-4030-8cf5-1b421fc45769",
      "metadata": {
        "id": "fdd11d65-56d8-4030-8cf5-1b421fc45769"
      },
      "source": [
        "#### Inference Video-MAE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "620e0ba8-7569-4662-a8a6-bc573a4fe916",
      "metadata": {
        "id": "620e0ba8-7569-4662-a8a6-bc573a4fe916"
      },
      "outputs": [],
      "source": [
        "del model, processor\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bb8d9fda-6457-4e11-b1ab-858e796780d7",
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "77cb304386ae44a9a0242da1871e5d67"
          ]
        },
        "id": "bb8d9fda-6457-4e11-b1ab-858e796780d7",
        "outputId": "2f6b0d19-b030-4bf0-ba23-991fe8895f95"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "77cb304386ae44a9a0242da1871e5d67",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/48 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "def extract_category_id(ctgry: str) -> int:\n",
        "    return int(float(ctgry[:3]))\n",
        "\n",
        "\n",
        "DATA_DIR = './data'\n",
        "model_name = 'videomae'\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "model, processor, window_size = instantiate_model(model_name, device)\n",
        "embeddings_videomae = []\n",
        "\n",
        "for data_subdir in tqdm.tqdm(os.listdir(DATA_DIR)):\n",
        "    fullpath = os.path.join(DATA_DIR, data_subdir)\n",
        "\n",
        "    if not os.path.isdir(fullpath):\n",
        "        continue\n",
        "\n",
        "    video_files = [f for f in os.listdir(fullpath) if f.lower().endswith('.mp4')]\n",
        "    logging.info(f\"Found {len(video_files)} video(s) in directory '{data_subdir}'.\")\n",
        "\n",
        "    for video_file in video_files:\n",
        "        file_path = os.path.join(fullpath, video_file)\n",
        "        output = process_video(file_path, processor, model, device, desired_frames=96, window_size=window_size)\n",
        "\n",
        "        if output is not None:\n",
        "            logging.info(f\"Successfully processed video: {video_file}\")\n",
        "            embeddings_videomae.append((video_file, extract_category_id(data_subdir), output.numpy()))\n",
        "        else:\n",
        "            logging.warning(f\"Failed to process video: {video_file}\")\n",
        "\n",
        "videomae_df = EmbeddingStorage(\n",
        "    filenames=list(map(lambda x: x[0], embeddings_videomae)),\n",
        "    labels=list(map(lambda x: x[1], embeddings_videomae)),\n",
        "    embeddings=list(map(lambda x: x[2], embeddings_videomae)),\n",
        ")\n",
        "videomae_df.save_to_file('data/embeddings/videomae.npz')\n",
        "\n",
        "# videomae_df = pd.DataFrame(data=embeddings_videomae, columns=['videoname', 'label', 'emb'])\n",
        "# videomae_df.to_csv('data/embeddings/videonae.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "78c46905-c5e7-4816-bbed-7b058044ea42",
      "metadata": {
        "id": "78c46905-c5e7-4816-bbed-7b058044ea42"
      },
      "source": [
        "## X-CLIP"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b4fccd24-5243-48a0-923f-bd863a40cfe7",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true,
        "id": "b4fccd24-5243-48a0-923f-bd863a40cfe7"
      },
      "source": [
        "### No context"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ea9bcb58-799a-401a-94c7-27765a51dcbb",
      "metadata": {
        "id": "ea9bcb58-799a-401a-94c7-27765a51dcbb"
      },
      "outputs": [],
      "source": [
        "tag_id_to_index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ca394c4b-de27-4b9b-8d08-232c3c970c49",
      "metadata": {
        "id": "ca394c4b-de27-4b9b-8d08-232c3c970c49"
      },
      "outputs": [],
      "source": [
        "clip_df = pd.read_csv('data/embeddings/x-clip-v2.csv')\n",
        "tag_id_to_index = dict(zip(label_df.tag_id, label_df.index))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2a82fc2b-1cbc-49d5-a65c-2ec9fea79aa3",
      "metadata": {
        "id": "2a82fc2b-1cbc-49d5-a65c-2ec9fea79aa3"
      },
      "source": [
        "Accuracy of first window"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "923c4954-968f-4c61-bd95-fdbe0840c206",
      "metadata": {
        "id": "923c4954-968f-4c61-bd95-fdbe0840c206",
        "outputId": "91fa3189-3317-49aa-99bc-5b81939c939e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Top 1 accuracy: 8.16831683168317\n",
            "Top 5 accuracy: 19.554455445544555\n",
            "Top 10 accuracy: 32.17821782178218\n"
          ]
        }
      ],
      "source": [
        "clip_id_label = clip_df.label.apply(lambda x: tag_id_to_index[x]).values\n",
        "top_n_predictions = np.vstack(clip_df.probs.apply(lambda x: np.argsort(string_to_numpy(x))[0, :][::-1]))\n",
        "\n",
        "print('Top 1 accuracy:', top_k_accuracy(clip_id_label, top_n_predictions, 1) * 100)\n",
        "print('Top 5 accuracy:', top_k_accuracy(clip_id_label, top_n_predictions, 5) * 100)\n",
        "print('Top 10 accuracy:', top_k_accuracy(clip_id_label, top_n_predictions, 10) * 100)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "85028f12-1bd4-4f88-9255-025f3390ccf6",
      "metadata": {
        "id": "85028f12-1bd4-4f88-9255-025f3390ccf6"
      },
      "source": [
        "Accuracy of second window"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ce3c969f-7afa-4068-aefd-d63e29631d49",
      "metadata": {
        "id": "ce3c969f-7afa-4068-aefd-d63e29631d49",
        "outputId": "e579b91b-5ea8-4b97-a741-2d29c574c297"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Top 1 accuracy: 9.158415841584159\n",
            "Top 5 accuracy: 20.049504950495052\n",
            "Top 10 accuracy: 31.18811881188119\n"
          ]
        }
      ],
      "source": [
        "clip_id_label = clip_df.label.apply(lambda x: tag_id_to_index[x]).values\n",
        "top_n_predictions = np.vstack(clip_df.probs.apply(lambda x: np.argsort(string_to_numpy(x))[1, :][::-1]))\n",
        "\n",
        "print('Top 1 accuracy:', top_k_accuracy(clip_id_label, top_n_predictions, 1) * 100)\n",
        "print('Top 5 accuracy:', top_k_accuracy(clip_id_label, top_n_predictions, 5) * 100)\n",
        "print('Top 10 accuracy:', top_k_accuracy(clip_id_label, top_n_predictions, 10) * 100)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "749a5bf8-8a11-4f28-906a-e9fad0c71e58",
      "metadata": {
        "id": "749a5bf8-8a11-4f28-906a-e9fad0c71e58"
      },
      "source": [
        "Accuracy of third window"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9d0a0079-0ee2-4b66-92fe-89ef9261a50d",
      "metadata": {
        "id": "9d0a0079-0ee2-4b66-92fe-89ef9261a50d",
        "outputId": "2586b997-a52a-43d4-e876-48fd6100b878"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Top 1 accuracy: 5.445544554455446\n",
            "Top 5 accuracy: 20.049504950495052\n",
            "Top 10 accuracy: 30.198019801980198\n"
          ]
        }
      ],
      "source": [
        "clip_id_label = clip_df.label.apply(lambda x: tag_id_to_index[x]).values\n",
        "top_n_predictions = np.vstack(clip_df.probs.apply(lambda x: np.argsort(string_to_numpy(x))[2, :][::-1]))\n",
        "\n",
        "print('Top 1 accuracy:', top_k_accuracy(clip_id_label, top_n_predictions, 1) * 100)\n",
        "print('Top 5 accuracy:', top_k_accuracy(clip_id_label, top_n_predictions, 5) * 100)\n",
        "print('Top 10 accuracy:', top_k_accuracy(clip_id_label, top_n_predictions, 10) * 100)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python (video-tag)",
      "language": "python",
      "name": "video-tag"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}