{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "95f80c2b-fbfe-4a7b-9f1f-f2651df49a1b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Импорты"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "75003a31-433f-4df1-a4c5-5a6cf0bf2ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchaudio\n",
    "\n",
    "import yt_dlp\n",
    "import subprocess\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "import gc\n",
    "import json\n",
    "import locale\n",
    "import re\n",
    "import tqdm.notebook as tqdm\n",
    "from urllib.parse import urlparse\n",
    "import requests\n",
    "import math\n",
    "import random\n",
    "\n",
    "import av\n",
    "from huggingface_hub import hf_hub_download\n",
    "from typing import Callable\n",
    "\n",
    "from transformers import VivitImageProcessor, VivitModel\n",
    "from transformers import AutoImageProcessor, VideoMAEModel\n",
    "from transformers import TimesformerConfig, TimesformerModel\n",
    "from transformers import XCLIPProcessor, XCLIPModel\n",
    "from transformers import AutoModelForSpeechSeq2Seq, AutoProcessor\n",
    "from transformers import Wav2Vec2ForCTC, Wav2Vec2Processor\n",
    "from transformers import BitsAndBytesConfig, LlavaNextVideoForConditionalGeneration, LlavaNextVideoProcessor\n",
    "from transformers import AutoModelForSpeechSeq2Seq, AutoProcessor, pipeline\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "import librosa\n",
    "from moviepy.editor import VideoFileClip\n",
    "import ast\n",
    "import openunmix\n",
    "\n",
    "from minio import Minio\n",
    "from minio.error import S3Error\n",
    "\n",
    "import hydra\n",
    "import soundfile as sf\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "import logging\n",
    "from typing import List\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.cross_decomposition import PLSRegression, CCA\n",
    "from sklearn.preprocessing import StandardScaler, LabelBinarizer\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, KFold\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from metric_learn import NCA\n",
    "from hiclass import MultiLabelLocalClassifierPerParentNode\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "locale.getpreferredencoding = lambda: \"UTF-8\"\n",
    "\n",
    "logging.basicConfig(\n",
    "    filename='vivit_inference.log',\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "\n",
    "SEED = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be0fef6d-d90a-48aa-8da8-df5212c4ee05",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Вспомогательные объекты"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "323c1d0a-d99a-4cf9-928a-01cff261e88f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingStorage:\n",
    "    def __init__(self, labels=None, filenames=None, embeddings=None):\n",
    "        \"\"\"\n",
    "        Initialize the EmbeddingStorage class.\n",
    "        \n",
    "        Args:\n",
    "            labels (list or np.ndarray): An array of labels for the embeddings.\n",
    "            filenames (list or np.ndarray): An array of filenames associated with the embeddings.\n",
    "            embeddings (np.ndarray): A NumPy array containing all embeddings.\n",
    "        \"\"\"\n",
    "        self.labels = np.array(labels) if labels is not None else np.array([])\n",
    "        self.filenames = np.array(filenames) if filenames is not None else np.array([])\n",
    "        self.embeddings = np.array(embeddings) if embeddings is not None else np.empty((0,))\n",
    "\n",
    "    def add_embedding(self, label, filename, embedding):\n",
    "        \"\"\"\n",
    "        Add a new embedding, along with its label and filename.\n",
    "        \n",
    "        Args:\n",
    "            label (int or str): The label of the embedding.\n",
    "            filename (str): The filename associated with the embedding.\n",
    "            embedding (np.ndarray or torch.Tensor): The embedding to add (can be a NumPy array or Tensor).\n",
    "        \"\"\"\n",
    "        if isinstance(embedding, np.ndarray):\n",
    "            emb_array = embedding\n",
    "        else:\n",
    "            # Convert torch.Tensor to NumPy\n",
    "            emb_array = embedding.cpu().numpy()\n",
    "        \n",
    "        # Append the new data\n",
    "        self.labels = np.append(self.labels, label)\n",
    "        self.filenames = np.append(self.filenames, filename)\n",
    "        \n",
    "        if self.embeddings.size == 0:\n",
    "            self.embeddings = emb_array.reshape(1, -1)\n",
    "        else:\n",
    "            self.embeddings = np.vstack([self.embeddings, emb_array])\n",
    "\n",
    "    def save_to_file(self, file_path):\n",
    "        np.savez(file_path, labels=self.labels, filenames=self.filenames, embeddings=self.embeddings)\n",
    "\n",
    "    @classmethod\n",
    "    def load_from_file(cls, file_path):\n",
    "        data = np.load(file_path, allow_pickle=True)\n",
    "        return cls(labels=data['labels'], filenames=data['filenames'], embeddings=data['embeddings'])\n",
    "\n",
    "    def get_embedding_by_filename(self, filename):\n",
    "        if filename in self.filenames:\n",
    "            idx = np.where(self.filenames == filename)[0][0]\n",
    "            return self.embeddings[idx]\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    def join_on_videoname(self, other_storage):\n",
    "        # Find common filenames\n",
    "        common_filenames = np.intersect1d(self.filenames, other_storage.filenames)\n",
    "        \n",
    "        # Initialize lists to store merged data\n",
    "        merged_labels = []\n",
    "        merged_filenames = []\n",
    "        merged_embeddings = []\n",
    "        \n",
    "        for filename in common_filenames:\n",
    "            # Get embeddings for the common filename from both storages\n",
    "            idx_self = np.where(self.filenames == filename)[0][0]\n",
    "            idx_other = np.where(other_storage.filenames == filename)[0][0]\n",
    "            \n",
    "            emb_self = self.embeddings[idx_self]\n",
    "            emb_other = other_storage.embeddings[idx_other]\n",
    "            \n",
    "            # Store embeddings as a tuple\n",
    "            merged_embedding = (emb_self, emb_other)\n",
    "            \n",
    "            # Get the label from the first storage (could be changed based on use case)\n",
    "            merged_label = self.labels[idx_self]\n",
    "            \n",
    "            # Append to the merged data\n",
    "            merged_labels.append(merged_label)\n",
    "            merged_filenames.append(filename)\n",
    "            merged_embeddings.append(merged_embedding)\n",
    "        \n",
    "        # Convert lists to numpy arrays\n",
    "        merged_labels = np.array(merged_labels)\n",
    "        merged_filenames = np.array(merged_filenames)\n",
    "        merged_embeddings = np.array(merged_embeddings, dtype=object)\n",
    "        \n",
    "        # Return a new EmbeddingStorage instance with merged data\n",
    "        return EmbeddingStorage(labels=merged_labels, filenames=merged_filenames, embeddings=merged_embeddings)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Return the number of embeddings stored.\n",
    "        \"\"\"\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Retrieve the label, filename, and embedding by index.\n",
    "        \n",
    "        Args:\n",
    "            idx (int): The index of the embedding to retrieve.\n",
    "        \n",
    "        Returns:\n",
    "            tuple: A tuple containing (label, filename, embedding).\n",
    "        \"\"\"\n",
    "        if idx >= len(self.labels):\n",
    "            raise IndexError(\"Index out of range\")\n",
    "        return self.labels[idx], self.filenames[idx], self.embeddings[idx]\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"EmbeddingStorage(labels={len(self.labels)}, filenames={len(self.filenames)}, embeddings_shape={self.embeddings.shape})\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5ba9d539-8465-4c7b-a0ff-7e7b92303251",
   "metadata": {},
   "outputs": [],
   "source": [
    "def iou_metric(ground_truth, predictions):\n",
    "    iou =  len(set.intersection(set(ground_truth), set(predictions)))\n",
    "    iou = iou/(len(set(ground_truth).union(set(predictions))))\n",
    "    return iou\n",
    "\n",
    "def split_tags(tag_list):\n",
    "    final_tag_list = []\n",
    "    for tag in tag_list:\n",
    "        tags = [tag.strip().lower() for tag in tag.split(\":\")]\n",
    "        if len(tags) == 3:\n",
    "            final_tag_list.append(tags[0])\n",
    "            final_tag_list.append(tags[0] + \": \" + tags[1])\n",
    "            final_tag_list.append(tags[0]+ \": \" + tags[1] + \": \" + tags[2])\n",
    "        elif len(tags) == 2:\n",
    "            final_tag_list.append(tags[0])\n",
    "            final_tag_list.append(tags[0] + \": \" + tags[1])\n",
    "        elif len(tags) == 1:\n",
    "            final_tag_list.append(tags[0])\n",
    "        else:\n",
    "            print(\"NOT IMPLEMENTED!!!!\", tag)\n",
    "    return final_tag_list\n",
    "\n",
    "\n",
    "def find_iou_for_sample_submission(pred_submission, true_submission):\n",
    "    ground_truth_df = true_submission\n",
    "    ground_truth_df[\"tags\"] = ground_truth_df[\"tags\"].apply(lambda l: l.split(', '))\n",
    "    ground_truth_df[\"tags_split\"] = ground_truth_df[\"tags\"].apply(lambda l: split_tags(l))\n",
    "\n",
    "    predictions_df = pred_submission\n",
    "    # predictions_df[\"predicted_tags\"] = predictions_df[\"predicted_tags\"].apply(ast.literal_eval)\n",
    "    predictions_df[\"predicted_tags_split\"] = predictions_df[\"predicted_tags\"].apply(lambda l: split_tags(l))\n",
    "    iou=0\n",
    "    counter = 0\n",
    "    for i, row in ground_truth_df.iterrows():\n",
    "        predicted_tags = predictions_df[predictions_df[\"video_id\"]==row[\"video_id\"]][\"predicted_tags_split\"].values[0]\n",
    "        iou_temp=iou_metric(row['tags_split'], predicted_tags)\n",
    "        iou+=iou_temp\n",
    "        counter+=1\n",
    "\n",
    "    return iou/counter\n",
    "\n",
    "\n",
    "def create_tags_to_labels(taxonomy):\n",
    "    tags = {}\n",
    "    for i, row in tqdm.tqdm(taxonomy.iterrows()):\n",
    "        if isinstance(row['Уровень 3 (iab)'], str):\n",
    "            tags[row['Уровень 1 (iab)'].strip().lower()+ \": \"+row['Уровень 2 (iab)'].strip().lower()+\": \"+row['Уровень 3 (iab)'].strip().lower()] = i\n",
    "        elif isinstance(row['Уровень 2 (iab)'], str):\n",
    "            tags[row['Уровень 1 (iab)'].strip().lower()+ \": \"+row['Уровень 2 (iab)'].strip().lower()] = i\n",
    "        elif isinstance(row['Уровень 1 (iab)'], str):\n",
    "            tags[row['Уровень 1 (iab)'].strip().lower()] = i\n",
    "    return tags"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c87ae226-bb66-4290-963f-41fc6704268f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Get description's embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dc939497-f8c6-4645-a06f-70fce73a73cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"train_data_categories.csv\", index_col=0)\n",
    "taxonomy = pd.read_csv(\"IAB_tags.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4a0107ec-4686-4f94-aa74-753e4e8142fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = SentenceTransformer('DeepPavlov/rubert-base-cased-sentence')\n",
    "# model.save_pretrained('models/rubert')\n",
    "# device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "# model = SentenceTransformer('models/rubert/')\n",
    "# model.to(device)\n",
    "# model.eval()\n",
    "# encoded_values = data['description'].apply(lambda l: model.encode(l, convert_to_tensor=True).cpu().numpy())\n",
    "# rubert_emb = EmbeddingStorage(filenames=data.index, embeddings=np.vstack(encoded_values.values))\n",
    "# rubert_emb.save_to_file('data/new_embeddings/description_emb.npz')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8305fab8-9d16-45ef-8fd7-60336da3d567",
   "metadata": {},
   "source": [
    "## Clear tags data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9a2035de-f825-44a6-99da-b84e3133c310",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "632f3566fe294b419ebfb1dc81e506eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tags_to_labels = create_tags_to_labels(taxonomy)\n",
    "labels_to_tags = {v: k for k, v in tags_to_labels.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "df42f103-334b-45d6-a3d4-1a68b46d05cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5fc5d743ca341419f020ab093856845",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty list\n",
      "Empty list\n",
      "Empty list\n",
      "Empty list\n"
     ]
    }
   ],
   "source": [
    "def assign_category_ids(data, tags_to_labels):\n",
    "    category_ids = []\n",
    "    \n",
    "    for i, row in tqdm.tqdm(data.iterrows()):\n",
    "        tags = row['tags'].split(', ')  # Split tags by comma\n",
    "        split_tag_list = split_tags(tags)  # Split hierarchical tags\n",
    "\n",
    "        # Convert the tags to category IDs\n",
    "        valid_ids = [tags_to_labels[tag] for tag in split_tag_list if tag in tags_to_labels]\n",
    "\n",
    "        if len(valid_ids) > 0:\n",
    "            # If valid category IDs are found, use all of them\n",
    "            category_ids.append(valid_ids)  # Append the list of valid IDs\n",
    "        else:\n",
    "            # If no valid tags, assign a random category\n",
    "            print(\"Empty list\", )\n",
    "            category_ids.append([random.choice(list(tags_to_labels.values()))])  # Random category from available tags\n",
    "\n",
    "    return category_ids\n",
    "\n",
    "\n",
    "data['category_id'] = assign_category_ids(data, tags_to_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0892dbe2-88ec-4d76-8ebb-5e53187c5c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "whisper_emb = EmbeddingStorage.load_from_file('data/new_embeddings/whisper.npz')\n",
    "rubert_emb = EmbeddingStorage.load_from_file('data/new_embeddings/description_emb.npz')\n",
    "xclip_emb = EmbeddingStorage.load_from_file('data/new_embeddings/xclip_emb.npz')\n",
    "xclip_emb.embeddings = xclip_emb.embeddings.mean(axis=1)\n",
    "\n",
    "data['audio_emb'] = [None] * len(data)\n",
    "data['text_emb'] = [None] * len(data)\n",
    "data['video_emb'] = [None] * len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1d478a7e-6ad0-42be-8ec2-b0a885c5ec96",
   "metadata": {},
   "outputs": [],
   "source": [
    "for filename, row in data.iterrows():\n",
    "    data.at[filename, 'video_emb'] = xclip_emb.get_embedding_by_filename(filename + '.mp4')\n",
    "    data.at[filename, 'audio_emb'] = whisper_emb.get_embedding_by_filename(filename + '.mp4')\n",
    "    data.at[filename, 'text_emb'] = rubert_emb.get_embedding_by_filename(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6007642-0a35-436c-b2d7-5f4dc423c72c",
   "metadata": {},
   "source": [
    "Do the same for augmented embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2d141cd9-ec1e-45ee-8b49-f5e93344bc52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "188b1b07c3454288bc61e44310d2e723",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty list\n",
      "Empty list\n",
      "Empty list\n",
      "Empty list\n"
     ]
    }
   ],
   "source": [
    "data_2 = pd.read_csv(\"train_data_categories.csv\", index_col=0)\n",
    "data_2['category_id'] = assign_category_ids(data_2, tags_to_labels)\n",
    "data_2['audio_emb'] = [None] * len(data)\n",
    "data_2['text_emb'] = [None] * len(data)\n",
    "data_2['video_emb'] = [None] * len(data)\n",
    "\n",
    "whisper_emb_augm = EmbeddingStorage.load_from_file('data/new_embeddings/augm_1/whisper.npz')\n",
    "rubert_emb_augm = EmbeddingStorage.load_from_file('data/new_embeddings/description_emb.npz')\n",
    "xclip_emb_augm = EmbeddingStorage.load_from_file('data/new_embeddings/augm_1/xclip_emb.npz')\n",
    "xclip_emb_augm.embeddings = xclip_emb_augm.embeddings.mean(axis=1)\n",
    "\n",
    "for filename, row in data_2.iterrows():\n",
    "    data_2.at[filename, 'video_emb'] = xclip_emb_augm.get_embedding_by_filename(filename + '.mp4')\n",
    "    data_2.at[filename, 'audio_emb'] = whisper_emb_augm.get_embedding_by_filename(filename + '.mp4')\n",
    "    data_2.at[filename, 'text_emb'] = rubert_emb_augm.get_embedding_by_filename(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "83c7f77a-1281-437e-a37e-c879bde3e0ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "X1 = np.vstack(data.audio_emb.values)\n",
    "X2 = np.vstack(data.text_emb.values)\n",
    "X3 = np.vstack(data.video_emb.values)\n",
    "Y = data.category_id.values\n",
    "\n",
    "X_a = np.vstack([X1, np.vstack(data_2.audio_emb.values)])\n",
    "X_t = np.vstack([X2, np.vstack(data_2.text_emb.values)])\n",
    "X_v = np.vstack([X3, np.vstack(data_2.video_emb.values)])\n",
    "Y_ttl = np.hstack([Y, data_2.category_id.values])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34f2c5d8-60c6-490a-9944-2275d18bb297",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Experiment with models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cdadb636-17f2-43e4-91f9-e5bc12f570e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_iou_from_ids(y_true, y_pred, labels_to_tags):\n",
    "    iou_scores = []\n",
    "    for true_labels, pred_labels in zip(y_true, y_pred):\n",
    "        true_labels_dec = split_tags([labels_to_tags[x] for x in true_labels])\n",
    "        pred_labels_dec = split_tags([labels_to_tags[x] for x in pred_labels])\n",
    "        \n",
    "        iou_score = iou_metric(true_labels_dec, pred_labels_dec)\n",
    "        iou_scores.append(iou_score)\n",
    "    return np.mean(iou_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "358eb060-8826-4319-b71d-95bd3633858d",
   "metadata": {},
   "source": [
    "### kNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ffe214e1-b168-4edf-a05f-41952dad86a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================== 1-NN ====================\n",
      "Modalities: all, iou: 0.5667, test_iou: 0.5710\n",
      "Modalities: audio + video, iou: 0.5064, test_iou: 0.5225\n",
      "Modalities: video + text, iou: 0.5500, test_iou: 0.5720\n",
      "Modalities: audio + text, iou: 0.5582, test_iou: 0.5600\n",
      "Modalities: audio, iou: 0.4754, test_iou: 0.4946\n",
      "Modalities: video, iou: 0.5841, test_iou: 0.5923\n",
      "Modalities: text, iou: 0.5407, test_iou: 0.5582\n",
      "==================== 3-NN ====================\n",
      "Modalities: all, iou: 0.5732, test_iou: 0.5824\n",
      "Modalities: audio + video, iou: 0.5098, test_iou: 0.5200\n",
      "Modalities: video + text, iou: 0.5575, test_iou: 0.5727\n",
      "Modalities: audio + text, iou: 0.5631, test_iou: 0.5751\n",
      "Modalities: audio, iou: 0.4845, test_iou: 0.4973\n",
      "Modalities: video, iou: 0.5995, test_iou: 0.6344\n",
      "Modalities: text, iou: 0.5423, test_iou: 0.5605\n",
      "==================== 5-NN ====================\n",
      "Modalities: all, iou: 0.5447, test_iou: 0.5809\n",
      "Modalities: audio + video, iou: 0.4803, test_iou: 0.5088\n",
      "Modalities: video + text, iou: 0.5313, test_iou: 0.5750\n",
      "Modalities: audio + text, iou: 0.5415, test_iou: 0.5805\n",
      "Modalities: audio, iou: 0.4488, test_iou: 0.4657\n",
      "Modalities: video, iou: 0.5899, test_iou: 0.6152\n",
      "Modalities: text, iou: 0.5232, test_iou: 0.5608\n"
     ]
    }
   ],
   "source": [
    "data_arr = [\n",
    "    np.hstack([X1, X2, X3]),\n",
    "    np.hstack([X1, X3]),\n",
    "    np.hstack([X2, X3]),\n",
    "    np.hstack([X1, X2]),\n",
    "    X1,\n",
    "    X3,\n",
    "    X2\n",
    "]\n",
    "\n",
    "data_types = [\n",
    "    'all',\n",
    "    'audio + video',\n",
    "    'video + text',\n",
    "    'audio + text',\n",
    "    'audio',\n",
    "    'video',\n",
    "    'text'\n",
    "]\n",
    "\n",
    "K_ARR = [1, 3, 5]\n",
    "\n",
    "np.random.seed(SEED)\n",
    "test_ratio = 0.2\n",
    "sample_size = data.shape[0]\n",
    "test_indices = np.random.choice(sample_size, int(test_ratio*sample_size), replace=False)\n",
    "train_indices = np.array(list(set(np.arange(sample_size)) - set(test_indices)))\n",
    "\n",
    "for K in K_ARR:\n",
    "    print(f'==================== {K}-NN ====================')\n",
    "    for data, modalities in zip(data_arr, data_types):\n",
    "        X = data[train_indices]\n",
    "        data_test = data[test_indices]\n",
    "        \n",
    "        mlb = MultiLabelBinarizer()\n",
    "        mlb.fit(Y)\n",
    "        Y_enc = mlb.transform(Y[train_indices])\n",
    "        Y_test = Y[test_indices]\n",
    "        \n",
    "        # Step 4: Define the cross-validation process\n",
    "        kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "        iou_scores = []\n",
    "        test_iou_scores = []\n",
    "        \n",
    "        for train_index, test_index in kf.split(X):\n",
    "            # Split the data into training and test sets for this fold\n",
    "            X_train, X_test = X[train_index], X[test_index]\n",
    "            y_train, y_test = Y_enc[train_index], Y_enc[test_index]\n",
    "            \n",
    "            # KNN classifier for multilabel classification\n",
    "            knn = KNeighborsClassifier(n_neighbors=K, p=1, weights='uniform')\n",
    "            multi_knn = MultiOutputClassifier(knn, n_jobs=-1)\n",
    "            multi_knn.fit(X_train, y_train)\n",
    "        \n",
    "            # Decode the predictions back to original multilabel format\n",
    "            y_pred = mlb.inverse_transform(multi_knn.predict(X_test))\n",
    "            y_test_decoded = mlb.inverse_transform(y_test)\n",
    "        \n",
    "            # Calculate IoU for this fold\n",
    "            fold_iou_score = calc_iou_from_ids(y_test_decoded, y_pred, labels_to_tags)\n",
    "            iou_scores.append(fold_iou_score)\n",
    "\n",
    "            Y_pred_test = mlb.inverse_transform(multi_knn.predict(data_test))\n",
    "            test_iou_scores.append(calc_iou_from_ids(Y_test, Y_pred_test, labels_to_tags))\n",
    "            \n",
    "        average_iou = np.mean(iou_scores)\n",
    "        test_iou = np.mean(test_iou_scores)\n",
    "        print(f\"Modalities: {modalities}, iou: {average_iou:.4f}, test_iou: {test_iou:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7efd2b18-cdaf-4ffa-bd0e-40578131f31d",
   "metadata": {},
   "source": [
    "Choose appropriate K and modality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7808d7a-96ff-4b6c-abcf-e8164da95dd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================== 1-NN ====================\n",
      "Modalities: all, iou: 0.9063\n",
      "Modalities: audio + video, iou: 0.6996\n",
      "Modalities: video + text, iou: 0.9080\n",
      "Modalities: audio + text, iou: 0.9036\n",
      "Modalities: audio, iou: 0.6249\n",
      "Modalities: video, iou: 0.9212\n",
      "Modalities: text, iou: 0.8949\n",
      "==================== 2-NN ====================\n",
      "Modalities: all, iou: 0.9063\n",
      "Modalities: audio + video, iou: 0.6997\n",
      "Modalities: video + text, iou: 0.9080\n",
      "Modalities: audio + text, iou: 0.9037\n",
      "Modalities: audio, iou: 0.6250\n",
      "Modalities: video, iou: 0.9212\n",
      "Modalities: text, iou: 0.8936\n",
      "==================== 3-NN ====================\n",
      "Modalities: all, iou: 0.9034\n",
      "Modalities: audio + video, iou: 0.6316\n",
      "Modalities: video + text, iou: 0.9065\n",
      "Modalities: audio + text, iou: 0.9020\n",
      "Modalities: audio, iou: 0.5754\n",
      "Modalities: video, iou: 0.8950\n",
      "Modalities: text, iou: 0.8971\n",
      "==================== 4-NN ====================\n",
      "Modalities: all, iou: 0.8943\n",
      "Modalities: audio + video, iou: 0.6306\n",
      "Modalities: video + text, iou: 0.9079\n",
      "Modalities: audio + text, iou: 0.8937\n",
      "Modalities: audio, iou: 0.5630\n",
      "Modalities: video, iou: 0.8652\n",
      "Modalities: text, iou: 0.8985\n",
      "==================== 5-NN ====================\n",
      "Modalities: all, iou: 0.8689\n",
      "Modalities: audio + video, iou: 0.5854\n"
     ]
    }
   ],
   "source": [
    "data_arr = [\n",
    "    np.hstack([X_a, X_v, X_t]),\n",
    "    np.hstack([X_a, X_v]),\n",
    "    np.hstack([X_v, X_t]),\n",
    "    np.hstack([X_a, X_t]),\n",
    "    X_a,\n",
    "    X_v,\n",
    "    X_t\n",
    "]\n",
    "\n",
    "data_types = [\n",
    "    'all',\n",
    "    'audio + video',\n",
    "    'video + text',\n",
    "    'audio + text',\n",
    "    'audio',\n",
    "    'video',\n",
    "    'text'\n",
    "]\n",
    "\n",
    "K_ARR = [1, 2, 3, 4, 5, 6]\n",
    "\n",
    "for K in K_ARR:\n",
    "    print(f'==================== {K}-NN ====================')\n",
    "    for data, modalities in zip(data_arr, data_types):\n",
    "        X = data\n",
    "        mlb = MultiLabelBinarizer()\n",
    "        Y_enc = mlb.fit_transform(Y_ttl)  # Y is your category_id, encoded as multilabel\n",
    "        \n",
    "        # Step 4: Define the cross-validation process\n",
    "        kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "        iou_scores = []\n",
    "        \n",
    "        for train_index, test_index in kf.split(X):\n",
    "            # Split the data into training and test sets for this fold\n",
    "            X_train, X_test = X[train_index], X[test_index]\n",
    "            y_train, y_test = Y_enc[train_index], Y_enc[test_index]\n",
    "            \n",
    "            # KNN classifier for multilabel classification\n",
    "            knn = KNeighborsClassifier(n_neighbors=K, p=1, weights='distance')\n",
    "            multi_knn = MultiOutputClassifier(knn, n_jobs=-1)\n",
    "            multi_knn.fit(X_train, y_train)\n",
    "        \n",
    "            # Predict using the KNN classifier\n",
    "            y_pred_enc = multi_knn.predict(X_test)\n",
    "        \n",
    "            # Decode the predictions back to original multilabel format\n",
    "            y_pred = mlb.inverse_transform(y_pred_enc)\n",
    "            y_test_decoded = mlb.inverse_transform(y_test)\n",
    "        \n",
    "            # Calculate IoU for this fold\n",
    "            fold_iou_score = calculate_iou(y_test_decoded, y_pred)\n",
    "            iou_scores.append(fold_iou_score)\n",
    "            \n",
    "        # Step 5: Calculate the average IoU across all folds\n",
    "        average_iou = np.mean(iou_scores)\n",
    "        print(f\"Modalities: {modalities}, iou: {average_iou:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8299ace6-93c2-46c7-a00a-b24ff67646f0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### ~~LogisticRegression~~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "79345679-dc8d-4338-821d-307e237484fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_arr = [\n",
    "#     np.hstack([X_a, X_v, X_t]),\n",
    "#     np.hstack([X_a, X_v]),\n",
    "#     np.hstack([X_v, X_t]),\n",
    "#     np.hstack([X_a, X_t]),\n",
    "#     X_a,\n",
    "#     X_v,\n",
    "#     X_t\n",
    "# ]\n",
    "\n",
    "# data_types = [\n",
    "#     'all',\n",
    "#     'audio + video',\n",
    "#     'video + text',\n",
    "#     'audio + text',\n",
    "#     'audio',\n",
    "#     'video',\n",
    "#     'text'\n",
    "# ]\n",
    "\n",
    "# C_ARR = [1e-3, 3e-3, 1e-2, 3e-2, 1e-1, 3e-1, 1, 3, 10]\n",
    "\n",
    "# for C in C_ARR:\n",
    "#     print(f'==================== LogisticRegression, C = {C} ====================')\n",
    "#     for data, modalities in zip(data_arr, data_types):\n",
    "#         X = data[train_indices]\n",
    "#         data_test = data[test_indices]\n",
    "        \n",
    "#         mlb = MultiLabelBinarizer()\n",
    "#         mlb.fit(Y_ttl)\n",
    "#         Y_enc = mlb.transform(Y_ttl[train_indices])\n",
    "#         Y_test = Y_ttl[test_indices]\n",
    "        \n",
    "#         # Step 4: Define the cross-validation process\n",
    "#         kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "#         iou_scores = []\n",
    "        \n",
    "#         for train_index, test_index in kf.split(X):\n",
    "#             # Split the data into training and test sets for this fold\n",
    "#             X_train, X_test = X[train_index], X[test_index]\n",
    "#             y_train, y_test = Y_enc[train_index], Y_enc[test_index]\n",
    "\n",
    "#             for i in range(y_train.shape[1]):\n",
    "#                 unique_classes = np.unique(y_train[:, i])\n",
    "#                 if len(unique_classes) < 2:\n",
    "#                     print(f\"Skipping fold for label {i} due to lack of class diversity.\")\n",
    "#                     continue  # Skip this fold if only one class is present\n",
    "            \n",
    "#             # KNN classifier for multilabel classification\n",
    "#             model = LogisticRegression(C=C, penalty='l1', max_iter=500, random_state=SEED, solver='saga')\n",
    "#             multi_model = MultiOutputClassifier(model, n_jobs=-1)\n",
    "#             multi_model.fit(X_train, y_train)\n",
    "        \n",
    "#             # Decode the predictions back to original multilabel format\n",
    "#             y_pred = mlb.inverse_transform(multi_model.predict(X_test))\n",
    "#             y_test_decoded = mlb.inverse_transform(y_test)\n",
    "        \n",
    "#             # Calculate IoU for this fold\n",
    "#             fold_iou_score = calc_iou_from_ids(y_test_decoded, y_pred, labels_to_tags)\n",
    "#             iou_scores.append(fold_iou_score)\n",
    "\n",
    "#         model = LogisticRegression(C=C, penalty='l1', max_iter=500, random_state=SEED, solver='saga')\n",
    "#         multi_model = MultiOutputClassifier(model, n_jobs=-1)\n",
    "#         multi_model.fit(X, Y_enc)\n",
    "#         Y_pred_test = mlb.inverse_transform(multi_model.predict(data_test))\n",
    "#         test_iou = calc_iou_from_ids(Y_test, Y_pred_test, labels_to_tags)\n",
    "            \n",
    "#         average_iou = np.mean(iou_scores)\n",
    "#         print(f\"Modalities: {modalities}, val_iou: {average_iou:.4f}, test_iou: {test_iou:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d46bf53-6923-4945-95ac-0ca282423e93",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### ~~CatBoostClassifier~~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e3144e14-bfa7-4d3e-bc47-1b37dc846899",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_arr = [\n",
    "#     np.hstack([X_a, X_v, X_t]),\n",
    "#     np.hstack([X_a, X_v]),\n",
    "#     np.hstack([X_v, X_t]),\n",
    "#     np.hstack([X_a, X_t]),\n",
    "#     X_a,\n",
    "#     X_v,\n",
    "#     X_t\n",
    "# ]\n",
    "\n",
    "# data_types = [\n",
    "#     'all',\n",
    "#     'audio + video',\n",
    "#     'video + text',\n",
    "#     'audio + text',\n",
    "#     'audio',\n",
    "#     'video',\n",
    "#     'text'\n",
    "# ]\n",
    "\n",
    "# for data, modalities in zip(data_arr, data_types):\n",
    "#     X = data[train_indices]\n",
    "#     data_test = data[test_indices]\n",
    "    \n",
    "#     mlb = MultiLabelBinarizer()\n",
    "#     mlb.fit(Y_ttl)\n",
    "#     Y_enc = mlb.transform(Y_ttl[train_indices])\n",
    "#     Y_test = Y_ttl[test_indices]\n",
    "    \n",
    "#     # Step 4: Define the cross-validation process\n",
    "#     kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "#     iou_scores = []\n",
    "    \n",
    "#     for train_index, test_index in kf.split(X):\n",
    "#         # Split the data into training and test sets for this fold\n",
    "#         X_train, X_test = X[train_index], X[test_index]\n",
    "#         y_train, y_test = Y_enc[train_index], Y_enc[test_index]\n",
    "\n",
    "#         # KNN classifier for multilabel classification\n",
    "#         model = CatBoostClassifier(learning_rate=0.01, iterations=100)\n",
    "#         multi_model = MultiOutputClassifier(model, n_jobs=-1)\n",
    "#         multi_model.fit(X_train, y_train)\n",
    "    \n",
    "#         # Decode the predictions back to original multilabel format\n",
    "#         y_pred = mlb.inverse_transform(multi_model.predict(X_test))\n",
    "#         y_test_decoded = mlb.inverse_transform(y_test)\n",
    "    \n",
    "#         # Calculate IoU for this fold\n",
    "#         fold_iou_score = calc_iou_from_ids(y_test_decoded, y_pred, labels_to_tags)\n",
    "#         iou_scores.append(fold_iou_score)\n",
    "\n",
    "#     model = CatBoostClassifier(learning_rate=0.01, iterations=100)\n",
    "#     multi_model = MultiOutputClassifier(model, n_jobs=-1)\n",
    "#     multi_model.fit(X, Y_enc)\n",
    "#     Y_pred_test = mlb.inverse_transform(multi_model.predict(data_test))\n",
    "#     test_iou = calc_iou_from_ids(Y_test, Y_pred_test, labels_to_tags)\n",
    "        \n",
    "#     average_iou = np.mean(iou_scores)\n",
    "#     print(f\"Modalities: {modalities}, val_iou: {average_iou:.4f}, test_iou: {test_iou:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67c6bac1-a3e9-4ca2-be80-33ef57912a4f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Local Classifier Per Parent Node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6456c9e2-21cc-4ff2-8902-513a73c596ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_ids_to_hierarchy(elem):\n",
    "    # Split each tag by ': ' and convert it into a list\n",
    "    tags = [labels_to_tags[ind] for ind in list(set(elem))]\n",
    "    hierarchy = [tag.split(\": \") for tag in tags]\n",
    "\n",
    "    return hierarchy\n",
    "\n",
    "\n",
    "def calc_iou_from_hierarchy(y_true, y_pred):\n",
    "    iou_scores = []\n",
    "    drop_empty_strings = lambda x: len(x) > 0\n",
    "    \n",
    "    for true_labels, pred_labels in zip(y_true, y_pred):\n",
    "        true_labels_dec = [': '.join(list(filter(drop_empty_strings, elem))) for elem in true_labels]\n",
    "        pred_labels_dec = [': '.join(list(filter(drop_empty_strings, elem))) for elem in pred_labels]\n",
    "\n",
    "        iou_score = iou_metric(split_tags(true_labels_dec), split_tags(pred_labels_dec))\n",
    "        iou_scores.append(iou_score)\n",
    "    return np.mean(iou_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3e97149-22df-4d6f-957b-83dfee3fc5bd",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### kNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "9ed26e75-8a4f-44bf-9177-8a37fa2764e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================== 1-NN ====================\n",
      "Modalities: all, val_iou: 0.6313, test_iou: 0.6655\n",
      "Modalities: audio + video, val_iou: 0.5256, test_iou: 0.5621\n",
      "Modalities: video + text, val_iou: 0.6240, test_iou: 0.6665\n",
      "Modalities: audio + text, val_iou: 0.6281, test_iou: 0.6673\n",
      "Modalities: audio, val_iou: 0.4676, test_iou: 0.5147\n",
      "Modalities: video, val_iou: 0.6340, test_iou: 0.6728\n",
      "Modalities: text, val_iou: 0.6204, test_iou: 0.6685\n"
     ]
    }
   ],
   "source": [
    "data_arr = [\n",
    "    np.hstack([X_a, X_v, X_t]),\n",
    "    np.hstack([X_a, X_v]),\n",
    "    np.hstack([X_v, X_t]),\n",
    "    np.hstack([X_a, X_t]),\n",
    "    X_a,\n",
    "    X_v,\n",
    "    X_t\n",
    "]\n",
    "\n",
    "data_types = [\n",
    "    'all',\n",
    "    'audio + video',\n",
    "    'video + text',\n",
    "    'audio + text',\n",
    "    'audio',\n",
    "    'video',\n",
    "    'text'\n",
    "]\n",
    "\n",
    "K_ARR = [1, 3, 5, 7, 9]\n",
    "\n",
    "np.random.seed(SEED)\n",
    "test_ratio = 0.2\n",
    "sample_size = X_a.shape[0]\n",
    "test_indices = np.random.choice(sample_size, int(test_ratio*sample_size), replace=False)\n",
    "train_indices = np.array(list(set(np.arange(sample_size)) - set(test_indices)))\n",
    "\n",
    "Y_enc = list(map(convert_ids_to_hierarchy, Y_ttl[train_indices]))\n",
    "Y_test = np.array(list(map(convert_ids_to_hierarchy, Y_ttl[test_indices])), dtype='object')\n",
    "\n",
    "for K in K_ARR:\n",
    "    print(f'==================== {K}-NN ====================')\n",
    "    for data, modalities in zip(data_arr, data_types):\n",
    "        X = data[train_indices]\n",
    "        data_test = data[test_indices]\n",
    "        \n",
    "        # Step 4: Define the cross-validation process\n",
    "        kf = KFold(n_splits=5, shuffle=True, random_state=SEED)\n",
    "        iou_scores = []\n",
    "        \n",
    "        for train_index, test_index in kf.split(X):\n",
    "            # Split the data into training and test sets for this fold\n",
    "            X_train, X_test = X[train_index], X[test_index]\n",
    "            y_train = np.array([Y_enc[ind] for ind in train_index], dtype=object)\n",
    "            y_test = np.array([Y_enc[ind] for ind in test_index], dtype=object)\n",
    "            \n",
    "            # KNN classifier for multilabel classification\n",
    "            knn = KNeighborsClassifier(n_neighbors=K, p=1, weights='distance')\n",
    "            classifier = MultiLabelLocalClassifierPerParentNode(local_classifier=knn, n_jobs=-1)\n",
    "            classifier.fit(X_train, y_train)\n",
    "        \n",
    "            # Decode the predictions back to original multilabel format\n",
    "            y_pred = classifier.predict(X_test)\n",
    "        \n",
    "            # Calculate IoU for this fold\n",
    "            fold_iou_score = calc_iou_from_hierarchy(y_test, y_pred)\n",
    "            iou_scores.append(fold_iou_score)\n",
    "\n",
    "        knn = KNeighborsClassifier(n_neighbors=K, p=1, weights='distance')\n",
    "        classifier = MultiLabelLocalClassifierPerParentNode(local_classifier=knn, n_jobs=-1)\n",
    "        classifier.fit(X, np.array(Y_enc, dtype='object'))\n",
    "        Y_pred_test = classifier.predict(data_test)\n",
    "        test_iou = calc_iou_from_hierarchy(Y_test, Y_pred_test)\n",
    "            \n",
    "        average_iou = np.mean(iou_scores)\n",
    "        print(f\"Modalities: {modalities}, val_iou: {average_iou:.4f}, test_iou: {test_iou:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3beb71a5-b9b7-4aae-be64-4b08c3f9e870",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "33701dee-7f66-4c1d-9712-5b9033f41c50",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# data_arr = [\n",
    "#     np.hstack([X_a, X_v, X_t]),\n",
    "#     np.hstack([X_a, X_v]),\n",
    "#     np.hstack([X_v, X_t]),\n",
    "#     np.hstack([X_a, X_t]),\n",
    "#     X_a,\n",
    "#     X_v,\n",
    "#     X_t\n",
    "# ]\n",
    "\n",
    "# data_types = [\n",
    "#     'all',\n",
    "#     'audio + video',\n",
    "#     'video + text',\n",
    "#     'audio + text',\n",
    "#     'audio',\n",
    "#     'video',\n",
    "#     'text'\n",
    "# ]\n",
    "\n",
    "# np.random.seed(SEED)\n",
    "# test_ratio = 0.2\n",
    "# sample_size = X_a.shape[0]\n",
    "# test_indices = np.random.choice(sample_size, int(test_ratio*sample_size), replace=False)\n",
    "# train_indices = np.array(list(set(np.arange(sample_size)) - set(test_indices)))\n",
    "\n",
    "# Y_enc = list(map(convert_ids_to_hierarchy, Y_ttl[train_indices]))\n",
    "# Y_test = np.array(list(map(convert_ids_to_hierarchy, Y_ttl[test_indices])), dtype='object')\n",
    "\n",
    "# C_ARR = [1e-3, 3e-3, 1e-2, 3e-2, 1e-1, 3e-1, 1, 3, 10]\n",
    "\n",
    "# for C in C_ARR:\n",
    "#     print(f'==================== LogisticRegression, C = {C} ====================')\n",
    "#     for data, modalities in zip(data_arr, data_types):\n",
    "#         X = data[train_indices]\n",
    "#         data_test = data[test_indices]\n",
    "        \n",
    "#         # Step 4: Define the cross-validation process\n",
    "#         kf = KFold(n_splits=5, shuffle=True, random_state=SEED)\n",
    "#         iou_scores = []\n",
    "        \n",
    "#         for train_index, test_index in kf.split(X):\n",
    "#             # Split the data into training and test sets for this fold\n",
    "#             X_train, X_test = X[train_index], X[test_index]\n",
    "#             y_train = np.array([Y_enc[ind] for ind in train_index], dtype=object)\n",
    "#             y_test = np.array([Y_enc[ind] for ind in test_index], dtype=object)\n",
    "            \n",
    "#             # KNN classifier for multilabel classification\n",
    "#             logreg = LogisticRegression(C=C, penalty='l1', max_iter=500, random_state=SEED, solver='saga')\n",
    "#             classifier = MultiLabelLocalClassifierPerParentNode(local_classifier=logreg, n_jobs=-1)\n",
    "#             classifier.fit(X_train, y_train)\n",
    "        \n",
    "#             # Decode the predictions back to original multilabel format\n",
    "#             y_pred = classifier.predict(X_test)\n",
    "        \n",
    "#             # Calculate IoU for this fold\n",
    "#             fold_iou_score = calc_iou_from_hierarchy(y_test, y_pred)\n",
    "#             iou_scores.append(fold_iou_score)\n",
    "\n",
    "#         logreg = LogisticRegression(C=C, penalty='l1', max_iter=500, random_state=SEED, solver='saga')\n",
    "#         classifier = MultiLabelLocalClassifierPerParentNode(local_classifier=logreg, n_jobs=-1)\n",
    "#         classifier.fit(X, np.array(Y_enc, dtype='object'))\n",
    "#         Y_pred_test = classifier.predict(data_test)\n",
    "#         test_iou = calc_iou_from_hierarchy(Y_test, Y_pred_test)\n",
    "            \n",
    "#         average_iou = np.mean(iou_scores)\n",
    "#         print(f\"Modalities: {modalities}, val_iou: {average_iou:.4f}, test_iou: {test_iou:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e84c5fd3-4da4-4d7e-aadf-8750ab91f40a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### CatBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e12c7130-7f0e-426c-acc1-589ebf1a643b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_arr = [\n",
    "#     np.hstack([X_a, X_v, X_t]),\n",
    "#     np.hstack([X_a, X_v]),\n",
    "#     np.hstack([X_v, X_t]),\n",
    "#     np.hstack([X_a, X_t]),\n",
    "#     X_a,\n",
    "#     X_v,\n",
    "#     X_t\n",
    "# ]\n",
    "\n",
    "# data_types = [\n",
    "#     'all',\n",
    "#     'audio + video',\n",
    "#     'video + text',\n",
    "#     'audio + text',\n",
    "#     'audio',\n",
    "#     'video',\n",
    "#     'text'\n",
    "# ]\n",
    "\n",
    "\n",
    "# np.random.seed(SEED)\n",
    "# test_ratio = 0.2\n",
    "# sample_size = X_a.shape[0]\n",
    "# test_indices = np.random.choice(sample_size, int(test_ratio*sample_size), replace=False)\n",
    "# train_indices = np.array(list(set(np.arange(sample_size)) - set(test_indices)))\n",
    "\n",
    "# Y_enc = list(map(convert_ids_to_hierarchy, Y_ttl[train_indices]))\n",
    "# Y_test = np.array(list(map(convert_ids_to_hierarchy, Y_ttl[test_indices])), dtype='object')\n",
    "\n",
    "# for data, modalities in zip(data_arr, data_types):\n",
    "#     X = data[train_indices]\n",
    "#     data_test = data[test_indices]\n",
    "    \n",
    "#     # Step 4: Define the cross-validation process\n",
    "#     kf = KFold(n_splits=5, shuffle=True, random_state=SEED)\n",
    "#     iou_scores = []\n",
    "    \n",
    "#     for train_index, test_index in kf.split(X):\n",
    "#         # Split the data into training and test sets for this fold\n",
    "#         X_train, X_test = X[train_index], X[test_index]\n",
    "#         y_train = np.array([Y_enc[ind] for ind in train_index], dtype=object)\n",
    "#         y_test = np.array([Y_enc[ind] for ind in test_index], dtype=object)\n",
    "        \n",
    "#         # KNN classifier for multilabel classification\n",
    "#         cb = CatBoostClassifier(learning_rate=0.01, iterations=100, verbose=False)\n",
    "#         classifier = MultiLabelLocalClassifierPerParentNode(local_classifier=cb, n_jobs=-1)\n",
    "#         classifier.fit(X_train, y_train)\n",
    "    \n",
    "#         # Decode the predictions back to original multilabel format\n",
    "#         y_pred = classifier.predict(X_test)\n",
    "    \n",
    "#         # Calculate IoU for this fold\n",
    "#         fold_iou_score = calc_iou_from_hierarchy(y_test, y_pred)\n",
    "#         iou_scores.append(fold_iou_score)\n",
    "\n",
    "#     cb = CatBoostClassifier(learning_rate=0.01, iterations=100, verbose=False)\n",
    "#     classifier = MultiLabelLocalClassifierPerParentNode(local_classifier=cb, n_jobs=-1)\n",
    "#     classifier.fit(X, np.array(Y_enc, dtype='object'))\n",
    "#     Y_pred_test = classifier.predict(data_test)\n",
    "#     test_iou = calc_iou_from_hierarchy(Y_test, Y_pred_test)\n",
    "        \n",
    "#     average_iou = np.mean(iou_scores)\n",
    "#     print(f\"Modalities: {modalities}, val_iou: {average_iou:.4f}, test_iou: {test_iou:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7bf144d-5986-4a05-a59d-e82d57586658",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Embeddings from outer-product matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1781f387-5e5b-4568-9bc7-0c5e551ff657",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_embedding_pytorch_chunked(X_a, X_v, X_t, chunk_size=64, device='cuda'):\n",
    "    # Convert the inputs to float32 and move data to GPU if available\n",
    "    X_a = torch.tensor(X_a, device=device, dtype=torch.float32)\n",
    "    X_v = torch.tensor(X_v, device=device, dtype=torch.float32)\n",
    "    X_t = torch.tensor(X_t, device=device, dtype=torch.float32)\n",
    "\n",
    "    # Initialize a list to collect all embeddings\n",
    "    embeddings = []\n",
    "\n",
    "    # Function to calculate statistics for input arrays\n",
    "    def compute_stats(arr):\n",
    "        stats = []\n",
    "        stats.append(torch.norm(arr, p=1, dim=1, keepdim=True))      # L1 norm\n",
    "        stats.append(torch.norm(arr, p=2, dim=1, keepdim=True))      # L2 norm\n",
    "        stats.append(torch.min(arr, dim=1, keepdim=True).values)     # Min\n",
    "        stats.append(torch.max(arr, dim=1, keepdim=True).values)     # Max\n",
    "        stats.append(torch.median(arr, dim=1, keepdim=True).values)  # Median\n",
    "        stats.append(torch.mean(arr, dim=1, keepdim=True))           # Mean\n",
    "        stats.append(torch.std(arr, dim=1, keepdim=True))            # Std\n",
    "        return stats\n",
    "\n",
    "    def compute_outer_product_stats(X_a_chunk, X_v_chunk, X_t_chunk):\n",
    "        outer_products = [\n",
    "            torch.bmm(X_a_chunk.unsqueeze(2), X_v_chunk.unsqueeze(1)),  # Outer product between X_a and X_v\n",
    "            torch.bmm(X_a_chunk.unsqueeze(2), X_t_chunk.unsqueeze(1)),  # Outer product between X_a and X_t\n",
    "            torch.bmm(X_v_chunk.unsqueeze(2), X_t_chunk.unsqueeze(1)),  # Outer product between X_v and X_t\n",
    "        ]\n",
    "        \n",
    "        # Compute statistics for each outer product\n",
    "        outer_prod_stats = []\n",
    "        for op in outer_products:\n",
    "            op_flat = op.view(op.shape[0], -1)  # Flatten each outer product for easier calculations\n",
    "            outer_prod_stats.extend(compute_stats(op_flat))\n",
    "        return outer_prod_stats\n",
    "\n",
    "    # Process in chunks to avoid memory issues\n",
    "    n = X_a.shape[0]\n",
    "    for i in range(0, n, chunk_size):\n",
    "        # Get the chunks of data\n",
    "        X_a_chunk = X_a[i:i+chunk_size]\n",
    "        X_v_chunk = X_v[i:i+chunk_size]\n",
    "        X_t_chunk = X_t[i:i+chunk_size]\n",
    "\n",
    "        # Compute statistics for input arrays\n",
    "        input_stats = []\n",
    "        for arr in [X_a_chunk, X_v_chunk, X_t_chunk]:\n",
    "            input_stats.extend(compute_stats(arr))\n",
    "        \n",
    "        # Compute outer product statistics in chunks\n",
    "        outer_prod_stats = compute_outer_product_stats(X_a_chunk, X_v_chunk, X_t_chunk)\n",
    "\n",
    "        # Concatenate all stats into a single tensor and store\n",
    "        chunk_embedding = torch.cat(input_stats + outer_prod_stats, dim=1)\n",
    "        embeddings.append(chunk_embedding.cpu())  # Move back to CPU to reduce GPU memory usage\n",
    "\n",
    "        # Free up memory\n",
    "        del X_a_chunk, X_v_chunk, X_t_chunk, input_stats, outer_prod_stats, chunk_embedding\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    # Concatenate all chunks into the final embedding matrix\n",
    "    return torch.cat(embeddings, dim=0).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9601e024-c372-4109-bc21-9044ab767a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = compute_embedding_pytorch_chunked(X_a, X_v, X_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa9af8a4-7c3e-423d-b02a-cc1906abb7b1",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Naive multilabel classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "18660af4-c578-41a9-bc99-1101a97b80e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================== 1-NN ====================\n",
      "Modalities: all, iou: 0.2233, test_iou: 0.2099\n",
      "==================== 3-NN ====================\n",
      "Modalities: all, iou: 0.2259, test_iou: 0.2066\n",
      "==================== 5-NN ====================\n",
      "Modalities: all, iou: 0.2169, test_iou: 0.1978\n",
      "==================== 7-NN ====================\n",
      "Modalities: all, iou: 0.2041, test_iou: 0.1963\n",
      "==================== 9-NN ====================\n",
      "Modalities: all, iou: 0.2029, test_iou: 0.1914\n"
     ]
    }
   ],
   "source": [
    "K_ARR = [1, 3, 5, 7, 9]\n",
    "\n",
    "np.random.seed(SEED)\n",
    "test_ratio = 0.2\n",
    "sample_size = embeddings.shape[0]\n",
    "test_indices = np.random.choice(sample_size, int(test_ratio*sample_size), replace=False)\n",
    "train_indices = np.array(list(set(np.arange(sample_size)) - set(test_indices)))\n",
    "\n",
    "for K in K_ARR:\n",
    "    print(f'==================== {K}-NN ====================')\n",
    "    X = embeddings[train_indices]\n",
    "    data_test = embeddings[test_indices]\n",
    "    \n",
    "    mlb = MultiLabelBinarizer()\n",
    "    mlb.fit(Y_ttl)\n",
    "    Y_enc = mlb.transform(Y_ttl[train_indices])\n",
    "    Y_test = Y_ttl[test_indices]\n",
    "    \n",
    "    # Step 4: Define the cross-validation process\n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    iou_scores = []\n",
    "    \n",
    "    for train_index, test_index in kf.split(X):\n",
    "        # Split the data into training and test sets for this fold\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = Y_enc[train_index], Y_enc[test_index]\n",
    "        \n",
    "        # KNN classifier for multilabel classification\n",
    "        knn = KNeighborsClassifier(n_neighbors=K, p=1, weights='distance')\n",
    "        multi_knn = MultiOutputClassifier(knn, n_jobs=-1)\n",
    "        multi_knn.fit(X_train, y_train)\n",
    "    \n",
    "        # Decode the predictions back to original multilabel format\n",
    "        y_pred = mlb.inverse_transform(multi_knn.predict(X_test))\n",
    "        y_test_decoded = mlb.inverse_transform(y_test)\n",
    "    \n",
    "        # Calculate IoU for this fold\n",
    "        fold_iou_score = calc_iou_from_ids(y_test_decoded, y_pred, labels_to_tags)\n",
    "        iou_scores.append(fold_iou_score)\n",
    "\n",
    "    knn = KNeighborsClassifier(n_neighbors=K, p=1, weights='distance')\n",
    "    multi_knn = MultiOutputClassifier(knn, n_jobs=-1)\n",
    "    multi_knn.fit(X, Y_enc)\n",
    "    Y_pred_test = mlb.inverse_transform(multi_knn.predict(data_test))\n",
    "    test_iou = calc_iou_from_ids(Y_test, Y_pred_test, labels_to_tags)\n",
    "\n",
    "    average_iou = np.mean(iou_scores)\n",
    "    print(f\"iou: {average_iou:.4f}, test_iou: {test_iou:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c4b65dc-219c-4ba9-ab47-753997a468b9",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Hierarchical multilabel classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52b79d99-9bf4-4b66-af58-fa04cb3e25aa",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### kNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "1e47b953-8824-46f2-9351-026d7013ba18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================== 1-NN ====================\n",
      "Modalities: all, val_iou: 0.2092, test_iou: 0.1899\n",
      "==================== 2-NN ====================\n",
      "Modalities: all, val_iou: 0.1658, test_iou: 0.1600\n"
     ]
    }
   ],
   "source": [
    "K_ARR = [1, 2]\n",
    "\n",
    "np.random.seed(SEED)\n",
    "test_ratio = 0.2\n",
    "sample_size = embeddings.shape[0]\n",
    "test_indices = np.random.choice(sample_size, int(test_ratio*sample_size), replace=False)\n",
    "train_indices = np.array(list(set(np.arange(sample_size)) - set(test_indices)))\n",
    "\n",
    "Y_enc = list(map(convert_ids_to_hierarchy, Y_ttl[train_indices]))\n",
    "Y_test = np.array(list(map(convert_ids_to_hierarchy, Y_ttl[test_indices])), dtype='object')\n",
    "\n",
    "for K in K_ARR:\n",
    "    print(f'==================== {K}-NN ====================')\n",
    "    X = embeddings[train_indices]\n",
    "    data_test = embeddings[test_indices]\n",
    "    \n",
    "    # Step 4: Define the cross-validation process\n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=SEED)\n",
    "    iou_scores = []\n",
    "    \n",
    "    for train_index, test_index in kf.split(X):\n",
    "        # Split the data into training and test sets for this fold\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train = np.array([Y_enc[ind] for ind in train_index], dtype=object)\n",
    "        y_test = np.array([Y_enc[ind] for ind in test_index], dtype=object)\n",
    "        \n",
    "        # KNN classifier for multilabel classification\n",
    "        knn = KNeighborsClassifier(n_neighbors=K, p=1, weights='distance')\n",
    "        classifier = MultiLabelLocalClassifierPerParentNode(local_classifier=knn, n_jobs=-1)\n",
    "        classifier.fit(X_train, y_train)\n",
    "    \n",
    "        # Decode the predictions back to original multilabel format\n",
    "        y_pred = classifier.predict(X_test)\n",
    "    \n",
    "        # Calculate IoU for this fold\n",
    "        fold_iou_score = calc_iou_from_hierarchy(y_test, y_pred)\n",
    "        iou_scores.append(fold_iou_score)\n",
    "\n",
    "    knn = KNeighborsClassifier(n_neighbors=K, p=1, weights='distance')\n",
    "    classifier = MultiLabelLocalClassifierPerParentNode(local_classifier=knn, n_jobs=-1)\n",
    "    classifier.fit(X, np.array(Y_enc, dtype='object'))\n",
    "    Y_pred_test = classifier.predict(data_test)\n",
    "    test_iou = calc_iou_from_hierarchy(Y_test, Y_pred_test)\n",
    "        \n",
    "    average_iou = np.mean(iou_scores)\n",
    "    print(f\"Modalities: {modalities}, val_iou: {average_iou:.4f}, test_iou: {test_iou:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32d1c8b2-e0f9-4e91-98c3-f94dc9e3faa0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "ed4bc226-9801-47db-a6a8-9ff2b37f4915",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================== LogisticRegression, C = 0.001 ====================\n",
      "Modalities: all, val_iou: 0.3374, test_iou: 0.3282\n",
      "==================== LogisticRegression, C = 0.003 ====================\n",
      "Modalities: all, val_iou: 0.3349, test_iou: 0.3334\n",
      "==================== LogisticRegression, C = 0.01 ====================\n",
      "Modalities: all, val_iou: 0.3379, test_iou: 0.3292\n",
      "==================== LogisticRegression, C = 0.03 ====================\n",
      "Modalities: all, val_iou: 0.3352, test_iou: 0.3260\n",
      "==================== LogisticRegression, C = 0.1 ====================\n",
      "Modalities: all, val_iou: 0.3362, test_iou: 0.3260\n",
      "==================== LogisticRegression, C = 0.3 ====================\n",
      "Modalities: all, val_iou: 0.3361, test_iou: 0.3276\n",
      "==================== LogisticRegression, C = 1 ====================\n",
      "Modalities: all, val_iou: 0.3350, test_iou: 0.3284\n",
      "==================== LogisticRegression, C = 3 ====================\n",
      "Modalities: all, val_iou: 0.3344, test_iou: 0.3310\n",
      "==================== LogisticRegression, C = 10 ====================\n",
      "Modalities: all, val_iou: 0.3364, test_iou: 0.3300\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(SEED)\n",
    "test_ratio = 0.2\n",
    "sample_size = embeddings.shape[0]\n",
    "test_indices = np.random.choice(sample_size, int(test_ratio*sample_size), replace=False)\n",
    "train_indices = np.array(list(set(np.arange(sample_size)) - set(test_indices)))\n",
    "\n",
    "Y_enc = list(map(convert_ids_to_hierarchy, Y_ttl[train_indices]))\n",
    "Y_test = np.array(list(map(convert_ids_to_hierarchy, Y_ttl[test_indices])), dtype='object')\n",
    "\n",
    "C_ARR = [1e-3, 3e-3, 1e-2, 3e-2, 1e-1, 3e-1, 1, 3, 10]\n",
    "\n",
    "for C in C_ARR:\n",
    "    print(f'==================== LogisticRegression, C = {C} ====================')\n",
    "    X = embeddings[train_indices]\n",
    "    data_test = embeddings[test_indices]\n",
    "    \n",
    "    # Step 4: Define the cross-validation process\n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=SEED)\n",
    "    iou_scores = []\n",
    "    \n",
    "    for train_index, test_index in kf.split(X):\n",
    "        # Split the data into training and test sets for this fold\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train = np.array([Y_enc[ind] for ind in train_index], dtype=object)\n",
    "        y_test = np.array([Y_enc[ind] for ind in test_index], dtype=object)\n",
    "        \n",
    "        # KNN classifier for multilabel classification\n",
    "        logreg = LogisticRegression(C=C, penalty='l2', max_iter=500, random_state=SEED)\n",
    "        classifier = MultiLabelLocalClassifierPerParentNode(local_classifier=logreg, n_jobs=-1)\n",
    "        classifier.fit(X_train, y_train)\n",
    "    \n",
    "        # Decode the predictions back to original multilabel format\n",
    "        y_pred = classifier.predict(X_test)\n",
    "    \n",
    "        # Calculate IoU for this fold\n",
    "        fold_iou_score = calc_iou_from_hierarchy(y_test, y_pred)\n",
    "        iou_scores.append(fold_iou_score)\n",
    "\n",
    "    logreg = LogisticRegression(C=C, penalty='l2', max_iter=500, random_state=SEED)\n",
    "    classifier = MultiLabelLocalClassifierPerParentNode(local_classifier=logreg, n_jobs=-1)\n",
    "    classifier.fit(X, np.array(Y_enc, dtype='object'))\n",
    "    Y_pred_test = classifier.predict(data_test)\n",
    "    test_iou = calc_iou_from_hierarchy(Y_test, Y_pred_test)\n",
    "        \n",
    "    average_iou = np.mean(iou_scores)\n",
    "    print(f\"Modalities: {modalities}, val_iou: {average_iou:.4f}, test_iou: {test_iou:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "d3f26d0f-d6b2-4242-825c-f5dc2eaa39ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modalities: all, val_iou: 0.3586, test_iou: 0.3518\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(SEED)\n",
    "test_ratio = 0.2\n",
    "sample_size = embeddings.shape[0]\n",
    "test_indices = np.random.choice(sample_size, int(test_ratio*sample_size), replace=False)\n",
    "train_indices = np.array(list(set(np.arange(sample_size)) - set(test_indices)))\n",
    "\n",
    "Y_enc = list(map(convert_ids_to_hierarchy, Y_ttl[train_indices]))\n",
    "Y_test = np.array(list(map(convert_ids_to_hierarchy, Y_ttl[test_indices])), dtype='object')\n",
    "\n",
    "X = embeddings[train_indices]\n",
    "data_test = embeddings[test_indices]\n",
    "\n",
    "# Step 4: Define the cross-validation process\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=SEED)\n",
    "iou_scores = []\n",
    "\n",
    "for train_index, test_index in kf.split(X):\n",
    "    # Split the data into training and test sets for this fold\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train = np.array([Y_enc[ind] for ind in train_index], dtype=object)\n",
    "    y_test = np.array([Y_enc[ind] for ind in test_index], dtype=object)\n",
    "    \n",
    "    # KNN classifier for multilabel classification\n",
    "    cb = CatBoostClassifier(learning_rate=0.01, iterations=100, verbose=False)\n",
    "    classifier = MultiLabelLocalClassifierPerParentNode(local_classifier=cb, n_jobs=-1)\n",
    "    classifier.fit(X_train, y_train)\n",
    "\n",
    "    # Decode the predictions back to original multilabel format\n",
    "    y_pred = classifier.predict(X_test)\n",
    "\n",
    "    # Calculate IoU for this fold\n",
    "    fold_iou_score = calc_iou_from_hierarchy(y_test, y_pred)\n",
    "    iou_scores.append(fold_iou_score)\n",
    "\n",
    "cb = CatBoostClassifier(learning_rate=0.01, iterations=100, verbose=False)\n",
    "classifier = MultiLabelLocalClassifierPerParentNode(local_classifier=cb, n_jobs=-1)\n",
    "classifier.fit(X, np.array(Y_enc, dtype='object'))\n",
    "Y_pred_test = classifier.predict(data_test)\n",
    "test_iou = calc_iou_from_hierarchy(Y_test, Y_pred_test)\n",
    "    \n",
    "average_iou = np.mean(iou_scores)\n",
    "print(f\"Modalities: {modalities}, val_iou: {average_iou:.4f}, test_iou: {test_iou:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d38cbc38-c56a-4c61-b3e3-82d5e0596e44",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## PLS-CCA experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4b11d38-0083-497a-aae0-1bdd897a8f25",
   "metadata": {},
   "source": [
    "**Methodology**\n",
    "\n",
    "1. CCA to each pair of domains + concatenation + embeddings\n",
    "2. CCA to one pair + concat with original vector + embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3e3b13da-66a6-4479-847f-8d4a23113cd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_cca(\n",
    "        X1: np.ndarray,\n",
    "        X2: np.ndarray,\n",
    "        train_indices: np.ndarray,\n",
    "        test_indices: np.ndarray,\n",
    "        n_comp: int = 32,\n",
    "        normalize: bool = True,\n",
    "        algo: str = 'pls'\n",
    "    ) -> dict:\n",
    "    X1_train_scaled = X1[train_indices]\n",
    "    X2_train_scaled = X2[train_indices]\n",
    "    \n",
    "    X1_test_scaled = X1[test_indices]\n",
    "    X2_test_scaled = X2[test_indices]\n",
    "\n",
    "    if algo == 'pls':\n",
    "        pls = PLSRegression(n_components=n_comp, scale=normalize)\n",
    "    elif algo == 'cca':\n",
    "        pls = CCA(n_components=n_comp, scale=normalize)\n",
    "\n",
    "    pls.fit(X1_train_scaled, X2_train_scaled)\n",
    "    \n",
    "    X1_latent_train, X2_latent_train = pls.transform(X1_train_scaled, X2_train_scaled)\n",
    "    X1_latent_test, X2_latent_test = pls.transform(X1_test_scaled, X2_test_scaled)\n",
    "    \n",
    "    latent_train = np.hstack([X1_latent_train, X2_latent_train])\n",
    "    latent_test = np.hstack([X1_latent_test, X2_latent_test])\n",
    "\n",
    "    return pls, latent_train, latent_test\n",
    "\n",
    "\n",
    "def concatenate_cca_embeddings(X_a, X_v, X_t, train_indices, test_indices, n_components=64, normalize=True, algo='pls'):\n",
    "    \"\"\"\n",
    "    Apply CCA between each pair of domains (audio, video, text) and concatenate the embeddings.\n",
    "    \n",
    "    Args:\n",
    "    X_a: Audio data\n",
    "    X_v: Video data\n",
    "    X_t: Text data\n",
    "    n_components: Number of CCA components to use\n",
    "    normalize: Whether to normalize the data\n",
    "    \n",
    "    Returns:\n",
    "    Concatenated latent embeddings from CCA on all pairs.\n",
    "    \"\"\"\n",
    "    # Run CCA on each pair of domains\n",
    "    pls_av, train_a_v, test_a_v = run_cca(X_a, X_v, train_indices, test_indices, n_components, normalize, algo=algo)\n",
    "    pls_at, train_a_t, test_a_t = run_cca(X_a, X_t, train_indices, test_indices, n_components, normalize, algo=algo)\n",
    "    pls_vt, train_v_t, test_v_t = run_cca(X_v, X_t, train_indices, test_indices, n_components, normalize, algo=algo)\n",
    "    \n",
    "    # Concatenate the embeddings from each CCA pair\n",
    "    latent_train = np.hstack([train_a_v, train_a_t, train_v_t])\n",
    "    latent_test = np.hstack([test_a_v, test_a_t, test_v_t])\n",
    "    \n",
    "    return (pls_av, pls_at, pls_vt), latent_train, latent_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0d0d25a-6d82-474c-9088-669c5a7f013d",
   "metadata": {},
   "source": [
    "### Method 1. Concatenation of 3 CCA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46cdfd1f-e797-4056-8b58-531666412e86",
   "metadata": {},
   "source": [
    "kNN, base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7c948c79-5d84-481f-9d8d-29cf65d6a0ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================== algo = pls, n_comp = 32 ====================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c84f39adde74c728a9518feecb7e5db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iou: 0.1589, test_iou: 0.6333\n",
      "==================== algo = pls, n_comp = 48 ====================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d29f84a5cce4adbb447cd0ad5dd3c87",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iou: 0.1621, test_iou: 0.6599\n",
      "==================== algo = pls, n_comp = 64 ====================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd4a13a23b464aa7b42d24d281359d40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iou: 0.1563, test_iou: 0.6313\n"
     ]
    }
   ],
   "source": [
    "K = 3\n",
    "\n",
    "np.random.seed(SEED)\n",
    "test_ratio = 0.2\n",
    "sample_size = Y.shape[0]\n",
    "test_indices = np.random.choice(sample_size, int(test_ratio*sample_size), replace=False)\n",
    "train_indices = np.array(list(set(np.arange(sample_size)) - set(test_indices)))\n",
    "\n",
    "mlb = MultiLabelBinarizer()\n",
    "mlb.fit(Y)\n",
    "Y_enc = mlb.transform(Y[train_indices])\n",
    "Y_test = Y[test_indices]\n",
    "algo = 'pls'\n",
    "\n",
    "for n_comp in [32, 48, 64]:\n",
    "    print(f'==================== algo = {algo}, n_comp = {n_comp} ====================')\n",
    "    kf = KFold(n_splits=4, shuffle=True, random_state=42)\n",
    "    iou_scores = []\n",
    "    \n",
    "    for train_index, test_index in tqdm.tqdm(kf.split(train_indices)):\n",
    "        # Split the data into training and test sets for this fold\n",
    "        y_train, y_test = Y_enc[train_index], Y_enc[test_index]\n",
    "\n",
    "        _, X_train, X_test = concatenate_cca_embeddings(X1, X3, X2, train_index, test_index, n_components=n_comp, algo=algo)\n",
    "        \n",
    "        # KNN classifier for multilabel classification\n",
    "        knn = KNeighborsClassifier(n_neighbors=K, p=1, weights='distance')\n",
    "        multi_knn = MultiOutputClassifier(knn, n_jobs=-1)\n",
    "        multi_knn.fit(X_train, y_train)\n",
    "    \n",
    "        # Decode the predictions back to original multilabel format\n",
    "        y_pred = mlb.inverse_transform(multi_knn.predict(X_test))\n",
    "        y_test_decoded = mlb.inverse_transform(y_test)\n",
    "    \n",
    "        # Calculate IoU for this fold\n",
    "        fold_iou_score = calc_iou_from_ids(y_test_decoded, y_pred, labels_to_tags)\n",
    "        iou_scores.append(fold_iou_score)\n",
    "\n",
    "    _, X, data_test = concatenate_cca_embeddings(X1, X3, X2, train_indices, test_indices, n_components=n_comp, algo=algo)\n",
    "    \n",
    "    knn = KNeighborsClassifier(n_neighbors=K, p=1, weights='distance')\n",
    "    multi_knn = MultiOutputClassifier(knn, n_jobs=-1)\n",
    "    multi_knn.fit(X, Y_enc)\n",
    "    Y_pred_test = mlb.inverse_transform(multi_knn.predict(data_test))\n",
    "    test_iou = calc_iou_from_ids(Y_test, Y_pred_test, labels_to_tags)\n",
    "\n",
    "    average_iou = np.mean(iou_scores)\n",
    "    print(f\"iou: {average_iou:.4f}, test_iou: {test_iou:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "c1e847f1-f363-42f8-a3e0-f7c95670d923",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================== algo = pls, n_comp = 32 ====================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c0e9255e8e349789dd78afc9bfc127c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modalities: all, iou: 0.1446, test_iou: 0.9081\n",
      "==================== algo = pls, n_comp = 48 ====================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0973a32e29894c87a6a72edc61359b65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modalities: all, iou: 0.1362, test_iou: 0.9089\n",
      "==================== algo = pls, n_comp = 64 ====================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed498ae95ad944dc91d92c0a6aeb599c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modalities: all, iou: 0.1382, test_iou: 0.9098\n",
      "==================== algo = cca, n_comp = 32 ====================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37880ac84800403e99cecf0621e139d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modalities: all, iou: 0.1422, test_iou: 0.7918\n",
      "==================== algo = cca, n_comp = 48 ====================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4159751a6e98449e87e3d47965f38907",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[84], line 24\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m train_index, test_index \u001b[38;5;129;01min\u001b[39;00m tqdm\u001b[38;5;241m.\u001b[39mtqdm(kf\u001b[38;5;241m.\u001b[39msplit(train_indices)):\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;66;03m# Split the data into training and test sets for this fold\u001b[39;00m\n\u001b[1;32m     22\u001b[0m     y_train, y_test \u001b[38;5;241m=\u001b[39m Y_enc[train_index], Y_enc[test_index]\n\u001b[0;32m---> 24\u001b[0m     _, X_train, X_test \u001b[38;5;241m=\u001b[39m \u001b[43mconcatenate_cca_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_a\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_v\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_t\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_components\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_comp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malgo\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43malgo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;66;03m# KNN classifier for multilabel classification\u001b[39;00m\n\u001b[1;32m     27\u001b[0m     knn \u001b[38;5;241m=\u001b[39m KNeighborsClassifier(n_neighbors\u001b[38;5;241m=\u001b[39mK, p\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, weights\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdistance\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[83], line 48\u001b[0m, in \u001b[0;36mconcatenate_cca_embeddings\u001b[0;34m(X_a, X_v, X_t, train_indices, test_indices, n_components, normalize, algo)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;66;03m# Run CCA on each pair of domains\u001b[39;00m\n\u001b[1;32m     47\u001b[0m pls_av, train_a_v, test_a_v \u001b[38;5;241m=\u001b[39m run_cca(X_a, X_v, train_indices, test_indices, n_components, normalize, algo\u001b[38;5;241m=\u001b[39malgo)\n\u001b[0;32m---> 48\u001b[0m pls_at, train_a_t, test_a_t \u001b[38;5;241m=\u001b[39m \u001b[43mrun_cca\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_a\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_t\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_indices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_indices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_components\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnormalize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malgo\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43malgo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     49\u001b[0m pls_vt, train_v_t, test_v_t \u001b[38;5;241m=\u001b[39m run_cca(X_v, X_t, train_indices, test_indices, n_components, normalize, algo\u001b[38;5;241m=\u001b[39malgo)\n\u001b[1;32m     51\u001b[0m \u001b[38;5;66;03m# Concatenate the embeddings from each CCA pair\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[83], line 21\u001b[0m, in \u001b[0;36mrun_cca\u001b[0;34m(X1, X2, train_indices, test_indices, n_comp, normalize, algo)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m algo \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcca\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m     19\u001b[0m     pls \u001b[38;5;241m=\u001b[39m CCA(n_components\u001b[38;5;241m=\u001b[39mn_comp, scale\u001b[38;5;241m=\u001b[39mnormalize)\n\u001b[0;32m---> 21\u001b[0m \u001b[43mpls\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX1_train_scaled\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX2_train_scaled\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m X1_latent_train, X2_latent_train \u001b[38;5;241m=\u001b[39m pls\u001b[38;5;241m.\u001b[39mtransform(X1_train_scaled, X2_train_scaled)\n\u001b[1;32m     24\u001b[0m X1_latent_test, X2_latent_test \u001b[38;5;241m=\u001b[39m pls\u001b[38;5;241m.\u001b[39mtransform(X1_test_scaled, X2_test_scaled)\n",
      "File \u001b[0;32m~/Рабочий стол/video-tagging/video-tag/lib/python3.10/site-packages/sklearn/base.py:1474\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1467\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1469\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1470\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1471\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1472\u001b[0m     )\n\u001b[1;32m   1473\u001b[0m ):\n\u001b[0;32m-> 1474\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Рабочий стол/video-tagging/video-tag/lib/python3.10/site-packages/sklearn/cross_decomposition/_pls.py:294\u001b[0m, in \u001b[0;36m_PLS.fit\u001b[0;34m(self, X, Y)\u001b[0m\n\u001b[1;32m    287\u001b[0m Yk[:, Yk_mask] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[1;32m    289\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    290\u001b[0m     (\n\u001b[1;32m    291\u001b[0m         x_weights,\n\u001b[1;32m    292\u001b[0m         y_weights,\n\u001b[1;32m    293\u001b[0m         n_iter_,\n\u001b[0;32m--> 294\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[43m_get_first_singular_vectors_power_method\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[43m        \u001b[49m\u001b[43mXk\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[43m        \u001b[49m\u001b[43mYk\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    297\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    298\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_iter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_iter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    299\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtol\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtol\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    300\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnorm_y_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnorm_y_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    301\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    302\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    303\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mY residual is constant\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m~/Рабочий стол/video-tagging/video-tag/lib/python3.10/site-packages/sklearn/cross_decomposition/_pls.py:104\u001b[0m, in \u001b[0;36m_get_first_singular_vectors_power_method\u001b[0;34m(X, Y, mode, max_iter, tol, norm_y_weights)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m norm_y_weights:\n\u001b[1;32m    102\u001b[0m     y_weights \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msqrt(np\u001b[38;5;241m.\u001b[39mdot(y_weights, y_weights)) \u001b[38;5;241m+\u001b[39m eps\n\u001b[0;32m--> 104\u001b[0m y_score \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mY\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_weights\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m/\u001b[39m (np\u001b[38;5;241m.\u001b[39mdot(y_weights, y_weights) \u001b[38;5;241m+\u001b[39m eps)\n\u001b[1;32m    106\u001b[0m x_weights_diff \u001b[38;5;241m=\u001b[39m x_weights \u001b[38;5;241m-\u001b[39m x_weights_old\n\u001b[1;32m    107\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39mdot(x_weights_diff, x_weights_diff) \u001b[38;5;241m<\u001b[39m tol \u001b[38;5;129;01mor\u001b[39;00m Y\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "K = 3\n",
    "\n",
    "np.random.seed(SEED)\n",
    "test_ratio = 0.2\n",
    "sample_size = embeddings.shape[0]\n",
    "test_indices = np.random.choice(sample_size, int(test_ratio*sample_size), replace=False)\n",
    "train_indices = np.array(list(set(np.arange(sample_size)) - set(test_indices)))\n",
    "\n",
    "mlb = MultiLabelBinarizer()\n",
    "mlb.fit(Y_ttl)\n",
    "Y_enc = mlb.transform(Y_ttl[train_indices])\n",
    "Y_test = Y_ttl[test_indices]\n",
    "algo = 'pls'\n",
    "\n",
    "for n_comp in [32, 48, 64]:\n",
    "    print(f'==================== algo = {algo}, n_comp = {n_comp} ====================')\n",
    "    kf = KFold(n_splits=4, shuffle=True, random_state=42)\n",
    "    iou_scores = []\n",
    "    \n",
    "    for train_index, test_index in tqdm.tqdm(kf.split(train_indices)):\n",
    "        # Split the data into training and test sets for this fold\n",
    "        y_train, y_test = Y_enc[train_index], Y_enc[test_index]\n",
    "\n",
    "        _, X_train, X_test = concatenate_cca_embeddings(X_a, X_v, X_t, train_index, test_index, n_components=n_comp, algo=algo)\n",
    "        \n",
    "        # KNN classifier for multilabel classification\n",
    "        knn = KNeighborsClassifier(n_neighbors=K, p=1, weights='distance')\n",
    "        multi_knn = MultiOutputClassifier(knn, n_jobs=-1)\n",
    "        multi_knn.fit(X_train, y_train)\n",
    "    \n",
    "        # Decode the predictions back to original multilabel format\n",
    "        y_pred = mlb.inverse_transform(multi_knn.predict(X_test))\n",
    "        y_test_decoded = mlb.inverse_transform(y_test)\n",
    "    \n",
    "        # Calculate IoU for this fold\n",
    "        fold_iou_score = calc_iou_from_ids(y_test_decoded, y_pred, labels_to_tags)\n",
    "        iou_scores.append(fold_iou_score)\n",
    "\n",
    "    _, X, data_test = concatenate_cca_embeddings(X_a, X_v, X_t, train_indices, test_indices, n_components=n_comp, algo=algo)\n",
    "    \n",
    "    knn = KNeighborsClassifier(n_neighbors=K, p=1, weights='distance')\n",
    "    multi_knn = MultiOutputClassifier(knn, n_jobs=-1)\n",
    "    multi_knn.fit(X, Y_enc)\n",
    "    Y_pred_test = mlb.inverse_transform(multi_knn.predict(data_test))\n",
    "    test_iou = calc_iou_from_ids(Y_test, Y_pred_test, labels_to_tags)\n",
    "\n",
    "    average_iou = np.mean(iou_scores)\n",
    "    print(f\"iou: {average_iou:.4f}, test_iou: {test_iou:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d175d29a-f06e-41cc-ba45-d4513847fb4f",
   "metadata": {},
   "source": [
    "knn, hierarchy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "03571cc0-fa30-43e8-8220-9e004041a570",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================== algo = pls, n_comp = 32 ====================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0d8d69a83584e0ba9fae4f61abf973a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iou: 0.1443, test_iou: 0.6680\n",
      "==================== algo = pls, n_comp = 48 ====================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "008f708f311c4a8794a4a08888ee7b16",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iou: 0.1401, test_iou: 0.6687\n",
      "==================== algo = pls, n_comp = 64 ====================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21589821de11413f8dbbc5c91e89a418",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iou: 0.1452, test_iou: 0.6722\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(SEED)\n",
    "test_ratio = 0.2\n",
    "sample_size = X_a.shape[0]\n",
    "test_indices = np.random.choice(sample_size, int(test_ratio*sample_size), replace=False)\n",
    "train_indices = np.array(list(set(np.arange(sample_size)) - set(test_indices)))\n",
    "\n",
    "Y_enc = list(map(convert_ids_to_hierarchy, Y_ttl[train_indices]))\n",
    "Y_test = np.array(list(map(convert_ids_to_hierarchy, Y_ttl[test_indices])), dtype='object')\n",
    "\n",
    "K = 1\n",
    "algo = 'pls'\n",
    "\n",
    "for n_comp in [32, 48, 64]:\n",
    "    print(f'==================== algo = {algo}, n_comp = {n_comp} ====================')\n",
    "    kf = KFold(n_splits=4, shuffle=True, random_state=42)\n",
    "    iou_scores = []\n",
    "    \n",
    "    for train_index, test_index in tqdm.tqdm(kf.split(train_indices)):\n",
    "        y_train = np.array([Y_enc[ind] for ind in train_index], dtype=object)\n",
    "        y_test = np.array([Y_enc[ind] for ind in test_index], dtype=object)\n",
    "\n",
    "        _, X_train, X_test = concatenate_cca_embeddings(X_a, X_v, X_t, train_index, test_index, n_components=n_comp, algo=algo)\n",
    "        \n",
    "        # KNN classifier for multilabel classification\n",
    "        knn = KNeighborsClassifier(n_neighbors=K, p=1, weights='distance')\n",
    "        classifier = MultiLabelLocalClassifierPerParentNode(local_classifier=knn, n_jobs=-1)\n",
    "        classifier.fit(X_train, y_train)\n",
    "    \n",
    "        y_pred = classifier.predict(X_test)\n",
    "    \n",
    "        # Calculate IoU for this fold\n",
    "        fold_iou_score = calc_iou_from_hierarchy(y_test, y_pred)\n",
    "        iou_scores.append(fold_iou_score)\n",
    "\n",
    "    _, X, data_test = concatenate_cca_embeddings(X_a, X_v, X_t, train_indices, test_indices, n_components=n_comp, algo=algo)\n",
    "    \n",
    "    knn = KNeighborsClassifier(n_neighbors=K, p=1, weights='distance')\n",
    "    classifier = MultiLabelLocalClassifierPerParentNode(local_classifier=knn, n_jobs=-1)\n",
    "    classifier.fit(X, np.array(Y_enc, dtype='object'))\n",
    "    Y_pred_test = classifier.predict(data_test)\n",
    "    test_iou = calc_iou_from_hierarchy(Y_test, Y_pred_test)\n",
    "    \n",
    "    average_iou = np.mean(iou_scores)\n",
    "    print(f\"iou: {average_iou:.4f}, test_iou: {test_iou:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d4b4c290-91b0-449d-8da4-3d94c8f580bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['музыка и аудио'], ['массовая культура']] === [['массовая культура' 'отношения знаменитостей' '']]\n",
      "[['хобби и интересы'], ['массовая культура']] === [['массовая культура' 'юмор и сатира' '']]\n",
      "[['массовая культура', 'юмор и сатира'], ['массовая культура']] === [['массовая культура' 'юмор и сатира' '']]\n",
      "[['спорт', 'борьба'], ['массовая культура'], ['спорт']] === [['массовая культура' 'юмор и сатира' '']]\n",
      "[['транспорт'], ['транспорт', 'авторемонт'], ['личные финансы'], ['массовая культура'], ['карьера']] === [['карьера' '' '']]\n",
      "[['религия и духовность'], ['религия и духовность', 'астрология']] === [['религия и духовность' 'астрология' '']]\n"
     ]
    }
   ],
   "source": [
    "for i in [12, 8, 14, 1, 0, 70]:\n",
    "    print(Y_test[i], '===', Y_pred_test[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c44d298f-dd0a-4342-83d9-0d83b1fc4b1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['религия и духовность'], ['религия и духовность', 'астрология']] === [['хобби и интересы' 'декоративно-прикладное искусство' 'живопись']]\n",
      "[['путешествия', 'направления путешествий'], ['путешествия', 'направления путешествий', 'азия'], ['путешествия']] === [['карьера' '' '']]\n",
      "[['изобразительное искусство'], ['хобби и интересы'], ['хобби и интересы', 'декоративно-прикладное искусство']] === [['карьера' '' '']]\n",
      "[['события и достопримечательности'], ['события и достопримечательности', 'исторические места и достопримечательности'], ['путешествия']] === [['карьера' '' '']]\n",
      "[['события и достопримечательности'], ['события и достопримечательности', 'спортивные события'], ['массовая культура', 'юмор и сатира'], ['массовая культура']] === [['карьера' '' '']]\n",
      "[['события и достопримечательности'], ['события и достопримечательности', 'исторические места и достопримечательности'], ['путешествия']] === [['фильмы и анимация' 'семейные и детские фильмы' '']]\n"
     ]
    }
   ],
   "source": [
    "for i in [12, 8, 14, 1, 0, 70]:\n",
    "    print(y_test[i], '===', y_pred[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "216d53ef-60fd-4e36-b9fe-965f05a0a396",
   "metadata": {},
   "source": [
    "### Method 2. 1 CCA with original features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1f06f407-6477-47ef-a057-01e6eadd3161",
   "metadata": {},
   "outputs": [],
   "source": [
    "def concatenate_cca_with_original(X_a, X_v, X_t, train_indices, test_indices, n_components=64, normalize=True, algo='pls'):\n",
    "    \"\"\"\n",
    "    Apply CCA between each pair of domains (audio, video, text), concatenate the CCA features \n",
    "    and the original input features.\n",
    "    \n",
    "    Args:\n",
    "    X_a: Audio data\n",
    "    X_v: Video data\n",
    "    X_t: Text data\n",
    "    n_components: Number of CCA components to use\n",
    "    normalize: Whether to normalize the data\n",
    "    \n",
    "    Returns:\n",
    "    Concatenated latent embeddings from CCA on all pairs plus original features.\n",
    "    \"\"\"\n",
    "    # Run CCA on each pair of domains\n",
    "    pls_av, train_a_v, test_a_v = run_cca(X_a, X_v, train_indices, test_indices, n_components, normalize, algo=algo)\n",
    "    train_t, text_t = X_t[train_indices], X_t[test_indices]\n",
    "    \n",
    "    # Concatenate CCA embeddings from each pair of domains\n",
    "    cca_train = np.hstack([train_a_v, train_t])\n",
    "    cca_test = np.hstack([test_a_v, text_t])\n",
    "    \n",
    "    return pls_av, cca_train, cca_test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f7ce7c6-d108-43f5-9fdf-15145bf0bfa2",
   "metadata": {},
   "source": [
    "kNN, base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "40505d6f-810f-4cd4-99b6-7343d4157228",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================== algo = pls, n_comp = 32 ====================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55ab81571a2e48baaae19639b16f3846",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_iou: 0.1438, test_iou: 0.9095\n",
      "==================== algo = pls, n_comp = 48 ====================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0fb8ac84c8541b9b4bccb59e0190622",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_iou: 0.1427, test_iou: 0.9181\n",
      "==================== algo = pls, n_comp = 64 ====================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17da16164b6342dfa410b6e1b749d5cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_iou: 0.1389, test_iou: 0.9184\n",
      "==================== algo = cca, n_comp = 32 ====================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43d32b0d46b14b92b378151123d7d8c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_iou: 0.1432, test_iou: 0.9016\n",
      "==================== algo = cca, n_comp = 48 ====================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc98aa066af847efb8f79641abe447c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[33], line 24\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m train_index, test_index \u001b[38;5;129;01min\u001b[39;00m tqdm\u001b[38;5;241m.\u001b[39mtqdm(kf\u001b[38;5;241m.\u001b[39msplit(train_indices)):\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;66;03m# Split the data into training and test sets for this fold\u001b[39;00m\n\u001b[1;32m     22\u001b[0m     y_train, y_test \u001b[38;5;241m=\u001b[39m Y_enc[train_index], Y_enc[test_index]\n\u001b[0;32m---> 24\u001b[0m     _, X_train, X_test \u001b[38;5;241m=\u001b[39m \u001b[43mconcatenate_cca_with_original\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_a\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_v\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_t\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_components\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_comp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malgo\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43malgo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;66;03m# KNN classifier for multilabel classification\u001b[39;00m\n\u001b[1;32m     27\u001b[0m     knn \u001b[38;5;241m=\u001b[39m KNeighborsClassifier(n_neighbors\u001b[38;5;241m=\u001b[39mK, p\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, weights\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdistance\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[30], line 17\u001b[0m, in \u001b[0;36mconcatenate_cca_with_original\u001b[0;34m(X_a, X_v, X_t, train_indices, test_indices, n_components, normalize, algo)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;03mApply CCA between each pair of domains (audio, video, text), concatenate the CCA features \u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124;03mand the original input features.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;124;03mConcatenated latent embeddings from CCA on all pairs plus original features.\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Run CCA on each pair of domains\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m pls_av, train_a_v, test_a_v \u001b[38;5;241m=\u001b[39m \u001b[43mrun_cca\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_a\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_v\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_indices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_indices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_components\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnormalize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malgo\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43malgo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m train_t, text_t \u001b[38;5;241m=\u001b[39m X_t[train_indices], X_t[test_indices]\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# Concatenate CCA embeddings from each pair of domains\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[15], line 21\u001b[0m, in \u001b[0;36mrun_cca\u001b[0;34m(X1, X2, train_indices, test_indices, n_comp, normalize, algo)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m algo \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcca\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m     19\u001b[0m     pls \u001b[38;5;241m=\u001b[39m CCA(n_components\u001b[38;5;241m=\u001b[39mn_comp, scale\u001b[38;5;241m=\u001b[39mnormalize)\n\u001b[0;32m---> 21\u001b[0m \u001b[43mpls\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX1_train_scaled\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX2_train_scaled\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m X1_latent_train, X2_latent_train \u001b[38;5;241m=\u001b[39m pls\u001b[38;5;241m.\u001b[39mtransform(X1_train_scaled, X2_train_scaled)\n\u001b[1;32m     24\u001b[0m X1_latent_test, X2_latent_test \u001b[38;5;241m=\u001b[39m pls\u001b[38;5;241m.\u001b[39mtransform(X1_test_scaled, X2_test_scaled)\n",
      "File \u001b[0;32m~/Рабочий стол/video-tagging/video-tag/lib/python3.10/site-packages/sklearn/base.py:1474\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1467\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1469\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1470\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1471\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1472\u001b[0m     )\n\u001b[1;32m   1473\u001b[0m ):\n\u001b[0;32m-> 1474\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Рабочий стол/video-tagging/video-tag/lib/python3.10/site-packages/sklearn/cross_decomposition/_pls.py:294\u001b[0m, in \u001b[0;36m_PLS.fit\u001b[0;34m(self, X, Y)\u001b[0m\n\u001b[1;32m    287\u001b[0m Yk[:, Yk_mask] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[1;32m    289\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    290\u001b[0m     (\n\u001b[1;32m    291\u001b[0m         x_weights,\n\u001b[1;32m    292\u001b[0m         y_weights,\n\u001b[1;32m    293\u001b[0m         n_iter_,\n\u001b[0;32m--> 294\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[43m_get_first_singular_vectors_power_method\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[43m        \u001b[49m\u001b[43mXk\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[43m        \u001b[49m\u001b[43mYk\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    297\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    298\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_iter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_iter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    299\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtol\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtol\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    300\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnorm_y_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnorm_y_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    301\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    302\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    303\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mY residual is constant\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m~/Рабочий стол/video-tagging/video-tag/lib/python3.10/site-packages/sklearn/cross_decomposition/_pls.py:85\u001b[0m, in \u001b[0;36m_get_first_singular_vectors_power_method\u001b[0;34m(X, Y, mode, max_iter, tol, norm_y_weights)\u001b[0m\n\u001b[1;32m     76\u001b[0m x_weights_old \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100\u001b[39m  \u001b[38;5;66;03m# init to big value for first convergence check\u001b[39;00m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mB\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m     79\u001b[0m     \u001b[38;5;66;03m# Precompute pseudo inverse matrices\u001b[39;00m\n\u001b[1;32m     80\u001b[0m     \u001b[38;5;66;03m# Basically: X_pinv = (X.T X)^-1 X.T\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     83\u001b[0m     \u001b[38;5;66;03m# B) will be unstable if n_features > n_samples or n_targets >\u001b[39;00m\n\u001b[1;32m     84\u001b[0m     \u001b[38;5;66;03m# n_samples\u001b[39;00m\n\u001b[0;32m---> 85\u001b[0m     X_pinv, Y_pinv \u001b[38;5;241m=\u001b[39m \u001b[43m_pinv2_old\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m, _pinv2_old(Y)\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(max_iter):\n\u001b[1;32m     88\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mB\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m~/Рабочий стол/video-tagging/video-tag/lib/python3.10/site-packages/sklearn/cross_decomposition/_pls.py:56\u001b[0m, in \u001b[0;36m_pinv2_old\u001b[0;34m(a)\u001b[0m\n\u001b[1;32m     54\u001b[0m u \u001b[38;5;241m=\u001b[39m u[:, :rank]\n\u001b[1;32m     55\u001b[0m u \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m=\u001b[39m s[:rank]\n\u001b[0;32m---> 56\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mtranspose(np\u001b[38;5;241m.\u001b[39mconjugate(\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mu\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvh\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43mrank\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m))\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "K = 3\n",
    "\n",
    "np.random.seed(SEED)\n",
    "test_ratio = 0.2\n",
    "sample_size = X_a.shape[0]\n",
    "test_indices = np.random.choice(sample_size, int(test_ratio*sample_size), replace=False)\n",
    "train_indices = np.array(list(set(np.arange(sample_size)) - set(test_indices)))\n",
    "\n",
    "mlb = MultiLabelBinarizer()\n",
    "mlb.fit(Y_ttl)\n",
    "Y_enc = mlb.transform(Y_ttl[train_indices])\n",
    "Y_test = Y_ttl[test_indices]\n",
    "\n",
    "algo = 'pls'\n",
    "\n",
    "for n_comp in [32, 48, 64]:\n",
    "    print(f'==================== algo = {algo}, n_comp = {n_comp} ====================')\n",
    "    kf = KFold(n_splits=4, shuffle=True, random_state=42)\n",
    "    iou_scores = []\n",
    "    \n",
    "    for train_index, test_index in tqdm.tqdm(kf.split(train_indices)):\n",
    "        # Split the data into training and test sets for this fold\n",
    "        y_train, y_test = Y_enc[train_index], Y_enc[test_index]\n",
    "\n",
    "        _, X_train, X_test = concatenate_cca_with_original(X_a, X_v, X_t, train_index, test_index, n_components=n_comp, algo=algo)\n",
    "        \n",
    "        # KNN classifier for multilabel classification\n",
    "        knn = KNeighborsClassifier(n_neighbors=K, p=1, weights='distance')\n",
    "        multi_knn = MultiOutputClassifier(knn, n_jobs=-1)\n",
    "        multi_knn.fit(X_train, y_train)\n",
    "    \n",
    "        # Decode the predictions back to original multilabel format\n",
    "        y_pred = mlb.inverse_transform(multi_knn.predict(X_test))\n",
    "        y_test_decoded = mlb.inverse_transform(y_test)\n",
    "    \n",
    "        # Calculate IoU for this fold\n",
    "        fold_iou_score = calc_iou_from_ids(y_test_decoded, y_pred, labels_to_tags)\n",
    "        iou_scores.append(fold_iou_score)\n",
    "\n",
    "    _, X, data_test = concatenate_cca_with_original(X_a, X_v, X_t, train_indices, test_indices, n_components=n_comp, algo=algo)\n",
    "    \n",
    "    knn = KNeighborsClassifier(n_neighbors=K, p=1, weights='distance')\n",
    "    multi_knn = MultiOutputClassifier(knn, n_jobs=-1)\n",
    "    multi_knn.fit(X, Y_enc)\n",
    "    Y_pred_test = mlb.inverse_transform(multi_knn.predict(data_test))\n",
    "    test_iou = calc_iou_from_ids(Y_test, Y_pred_test, labels_to_tags)\n",
    "\n",
    "    average_iou = np.mean(iou_scores)\n",
    "    print(f\"val_iou: {average_iou:.4f}, test_iou: {test_iou:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "914b0182-744f-49f4-8dcf-3b6c6350c34c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================== algo = pls, n_comp = 32 ====================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07b0bba509b84f4397e4520ad4df3b58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_iou: 0.1378, test_iou: 0.8975\n",
      "==================== algo = pls, n_comp = 48 ====================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "266e3997a37047bc845333b9d80f1059",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_iou: 0.1386, test_iou: 0.9028\n",
      "==================== algo = pls, n_comp = 64 ====================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8053279e9d9c47f48a370c7c845a374d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_iou: 0.1424, test_iou: 0.9049\n",
      "==================== algo = cca, n_comp = 32 ====================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d8fe24609ca451fa45f702549831f83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_iou: 0.1424, test_iou: 0.8696\n",
      "==================== algo = cca, n_comp = 48 ====================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04da31bed2934b1f9d4b2318f49826fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_iou: 0.1428, test_iou: 0.8280\n",
      "==================== algo = cca, n_comp = 64 ====================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa546dfaa12946d9a7bab51636fd0aae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[37], line 24\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m train_index, test_index \u001b[38;5;129;01min\u001b[39;00m tqdm\u001b[38;5;241m.\u001b[39mtqdm(kf\u001b[38;5;241m.\u001b[39msplit(train_indices)):\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;66;03m# Split the data into training and test sets for this fold\u001b[39;00m\n\u001b[1;32m     22\u001b[0m     y_train, y_test \u001b[38;5;241m=\u001b[39m Y_enc[train_index], Y_enc[test_index]\n\u001b[0;32m---> 24\u001b[0m     _, X_train, X_test \u001b[38;5;241m=\u001b[39m \u001b[43mconcatenate_cca_with_original\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_a\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_t\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_v\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_components\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_comp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malgo\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43malgo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;66;03m# KNN classifier for multilabel classification\u001b[39;00m\n\u001b[1;32m     27\u001b[0m     knn \u001b[38;5;241m=\u001b[39m KNeighborsClassifier(n_neighbors\u001b[38;5;241m=\u001b[39mK, p\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, weights\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdistance\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[30], line 17\u001b[0m, in \u001b[0;36mconcatenate_cca_with_original\u001b[0;34m(X_a, X_v, X_t, train_indices, test_indices, n_components, normalize, algo)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;03mApply CCA between each pair of domains (audio, video, text), concatenate the CCA features \u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124;03mand the original input features.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;124;03mConcatenated latent embeddings from CCA on all pairs plus original features.\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Run CCA on each pair of domains\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m pls_av, train_a_v, test_a_v \u001b[38;5;241m=\u001b[39m \u001b[43mrun_cca\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_a\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_v\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_indices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_indices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_components\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnormalize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malgo\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43malgo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m train_t, text_t \u001b[38;5;241m=\u001b[39m X_t[train_indices], X_t[test_indices]\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# Concatenate CCA embeddings from each pair of domains\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[15], line 21\u001b[0m, in \u001b[0;36mrun_cca\u001b[0;34m(X1, X2, train_indices, test_indices, n_comp, normalize, algo)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m algo \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcca\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m     19\u001b[0m     pls \u001b[38;5;241m=\u001b[39m CCA(n_components\u001b[38;5;241m=\u001b[39mn_comp, scale\u001b[38;5;241m=\u001b[39mnormalize)\n\u001b[0;32m---> 21\u001b[0m \u001b[43mpls\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX1_train_scaled\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX2_train_scaled\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m X1_latent_train, X2_latent_train \u001b[38;5;241m=\u001b[39m pls\u001b[38;5;241m.\u001b[39mtransform(X1_train_scaled, X2_train_scaled)\n\u001b[1;32m     24\u001b[0m X1_latent_test, X2_latent_test \u001b[38;5;241m=\u001b[39m pls\u001b[38;5;241m.\u001b[39mtransform(X1_test_scaled, X2_test_scaled)\n",
      "File \u001b[0;32m~/Рабочий стол/video-tagging/video-tag/lib/python3.10/site-packages/sklearn/base.py:1474\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1467\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1469\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1470\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1471\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1472\u001b[0m     )\n\u001b[1;32m   1473\u001b[0m ):\n\u001b[0;32m-> 1474\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Рабочий стол/video-tagging/video-tag/lib/python3.10/site-packages/sklearn/cross_decomposition/_pls.py:294\u001b[0m, in \u001b[0;36m_PLS.fit\u001b[0;34m(self, X, Y)\u001b[0m\n\u001b[1;32m    287\u001b[0m Yk[:, Yk_mask] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[1;32m    289\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    290\u001b[0m     (\n\u001b[1;32m    291\u001b[0m         x_weights,\n\u001b[1;32m    292\u001b[0m         y_weights,\n\u001b[1;32m    293\u001b[0m         n_iter_,\n\u001b[0;32m--> 294\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[43m_get_first_singular_vectors_power_method\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[43m        \u001b[49m\u001b[43mXk\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[43m        \u001b[49m\u001b[43mYk\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    297\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    298\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_iter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_iter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    299\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtol\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtol\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    300\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnorm_y_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnorm_y_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    301\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    302\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    303\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mY residual is constant\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m~/Рабочий стол/video-tagging/video-tag/lib/python3.10/site-packages/sklearn/cross_decomposition/_pls.py:85\u001b[0m, in \u001b[0;36m_get_first_singular_vectors_power_method\u001b[0;34m(X, Y, mode, max_iter, tol, norm_y_weights)\u001b[0m\n\u001b[1;32m     76\u001b[0m x_weights_old \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100\u001b[39m  \u001b[38;5;66;03m# init to big value for first convergence check\u001b[39;00m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mB\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m     79\u001b[0m     \u001b[38;5;66;03m# Precompute pseudo inverse matrices\u001b[39;00m\n\u001b[1;32m     80\u001b[0m     \u001b[38;5;66;03m# Basically: X_pinv = (X.T X)^-1 X.T\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     83\u001b[0m     \u001b[38;5;66;03m# B) will be unstable if n_features > n_samples or n_targets >\u001b[39;00m\n\u001b[1;32m     84\u001b[0m     \u001b[38;5;66;03m# n_samples\u001b[39;00m\n\u001b[0;32m---> 85\u001b[0m     X_pinv, Y_pinv \u001b[38;5;241m=\u001b[39m _pinv2_old(X), \u001b[43m_pinv2_old\u001b[49m\u001b[43m(\u001b[49m\u001b[43mY\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(max_iter):\n\u001b[1;32m     88\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mB\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m~/Рабочий стол/video-tagging/video-tag/lib/python3.10/site-packages/sklearn/cross_decomposition/_pls.py:47\u001b[0m, in \u001b[0;36m_pinv2_old\u001b[0;34m(a)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_pinv2_old\u001b[39m(a):\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;66;03m# Used previous scipy pinv2 that was updated in:\u001b[39;00m\n\u001b[1;32m     43\u001b[0m     \u001b[38;5;66;03m# https://github.com/scipy/scipy/pull/10067\u001b[39;00m\n\u001b[1;32m     44\u001b[0m     \u001b[38;5;66;03m# We can not set `cond` or `rcond` for pinv2 in scipy >= 1.3 to keep the\u001b[39;00m\n\u001b[1;32m     45\u001b[0m     \u001b[38;5;66;03m# same behavior of pinv2 for scipy < 1.3, because the condition used to\u001b[39;00m\n\u001b[1;32m     46\u001b[0m     \u001b[38;5;66;03m# determine the rank is dependent on the output of svd.\u001b[39;00m\n\u001b[0;32m---> 47\u001b[0m     u, s, vh \u001b[38;5;241m=\u001b[39m \u001b[43msvd\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfull_matrices\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheck_finite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     49\u001b[0m     t \u001b[38;5;241m=\u001b[39m u\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mchar\u001b[38;5;241m.\u001b[39mlower()\n\u001b[1;32m     50\u001b[0m     factor \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m1e3\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124md\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m1e6\u001b[39m}\n",
      "File \u001b[0;32m~/Рабочий стол/video-tagging/video-tag/lib/python3.10/site-packages/scipy/linalg/_decomp_svd.py:127\u001b[0m, in \u001b[0;36msvd\u001b[0;34m(a, full_matrices, compute_uv, overwrite_a, check_finite, lapack_driver)\u001b[0m\n\u001b[1;32m    123\u001b[0m lwork \u001b[38;5;241m=\u001b[39m _compute_lwork(gesXd_lwork, a1\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], a1\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m],\n\u001b[1;32m    124\u001b[0m                        compute_uv\u001b[38;5;241m=\u001b[39mcompute_uv, full_matrices\u001b[38;5;241m=\u001b[39mfull_matrices)\n\u001b[1;32m    126\u001b[0m \u001b[38;5;66;03m# perform decomposition\u001b[39;00m\n\u001b[0;32m--> 127\u001b[0m u, s, v, info \u001b[38;5;241m=\u001b[39m \u001b[43mgesXd\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcompute_uv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompute_uv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlwork\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlwork\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    128\u001b[0m \u001b[43m                      \u001b[49m\u001b[43mfull_matrices\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfull_matrices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moverwrite_a\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moverwrite_a\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m info \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    131\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m LinAlgError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSVD did not converge\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "K = 3\n",
    "\n",
    "np.random.seed(SEED)\n",
    "test_ratio = 0.2\n",
    "sample_size = X_a.shape[0]\n",
    "test_indices = np.random.choice(sample_size, int(test_ratio*sample_size), replace=False)\n",
    "train_indices = np.array(list(set(np.arange(sample_size)) - set(test_indices)))\n",
    "\n",
    "mlb = MultiLabelBinarizer()\n",
    "mlb.fit(Y_ttl)\n",
    "Y_enc = mlb.transform(Y_ttl[train_indices])\n",
    "Y_test = Y_ttl[test_indices]\n",
    "\n",
    "for algo in ('pls', 'cca'):\n",
    "    for n_comp in [32, 48, 64]:\n",
    "        print(f'==================== algo = {algo}, n_comp = {n_comp} ====================')\n",
    "        kf = KFold(n_splits=4, shuffle=True, random_state=42)\n",
    "        iou_scores = []\n",
    "        \n",
    "        for train_index, test_index in tqdm.tqdm(kf.split(train_indices)):\n",
    "            # Split the data into training and test sets for this fold\n",
    "            y_train, y_test = Y_enc[train_index], Y_enc[test_index]\n",
    "    \n",
    "            _, X_train, X_test = concatenate_cca_with_original(X_a, X_t, X_v, train_index, test_index, n_components=n_comp, algo=algo)\n",
    "            \n",
    "            # KNN classifier for multilabel classification\n",
    "            knn = KNeighborsClassifier(n_neighbors=K, p=1, weights='distance')\n",
    "            multi_knn = MultiOutputClassifier(knn, n_jobs=-1)\n",
    "            multi_knn.fit(X_train, y_train)\n",
    "        \n",
    "            # Decode the predictions back to original multilabel format\n",
    "            y_pred = mlb.inverse_transform(multi_knn.predict(X_test))\n",
    "            y_test_decoded = mlb.inverse_transform(y_test)\n",
    "        \n",
    "            # Calculate IoU for this fold\n",
    "            fold_iou_score = calc_iou_from_ids(y_test_decoded, y_pred, labels_to_tags)\n",
    "            iou_scores.append(fold_iou_score)\n",
    "    \n",
    "        _, X, data_test = concatenate_cca_with_original(X_a, X_t, X_v, train_indices, test_indices, n_components=n_comp, algo=algo)\n",
    "        \n",
    "        knn = KNeighborsClassifier(n_neighbors=K, p=1, weights='distance')\n",
    "        multi_knn = MultiOutputClassifier(knn, n_jobs=-1)\n",
    "        multi_knn.fit(X, Y_enc)\n",
    "        Y_pred_test = mlb.inverse_transform(multi_knn.predict(data_test))\n",
    "        test_iou = calc_iou_from_ids(Y_test, Y_pred_test, labels_to_tags)\n",
    "    \n",
    "        average_iou = np.mean(iou_scores)\n",
    "        print(f\"val_iou: {average_iou:.4f}, test_iou: {test_iou:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58358784-b469-439b-9ee1-bb624a0bc723",
   "metadata": {},
   "source": [
    "knn, hierarchy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "76beaee2-3302-4b5f-bfec-571c2e52bb2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================== algo = pls, n_comp = 32 ====================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7277ae645c042ce93a6a1ace227ef77",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iou: 0.1482, test_iou: 0.6733\n",
      "==================== algo = pls, n_comp = 48 ====================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2cf6fe70c7f24a6bb114b8cb7d3554fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iou: 0.1451, test_iou: 0.6698\n",
      "==================== algo = pls, n_comp = 64 ====================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59a95772ada34131bd1dc400ad679605",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iou: 0.1496, test_iou: 0.6762\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(SEED)\n",
    "test_ratio = 0.2\n",
    "sample_size = X_a.shape[0]\n",
    "test_indices = np.random.choice(sample_size, int(test_ratio*sample_size), replace=False)\n",
    "train_indices = np.array(list(set(np.arange(sample_size)) - set(test_indices)))\n",
    "\n",
    "Y_enc = list(map(convert_ids_to_hierarchy, Y_ttl[train_indices]))\n",
    "Y_test = np.array(list(map(convert_ids_to_hierarchy, Y_ttl[test_indices])), dtype='object')\n",
    "\n",
    "K = 1\n",
    "algo = 'pls'\n",
    "\n",
    "for n_comp in [32, 48, 64]:\n",
    "    print(f'==================== algo = {algo}, n_comp = {n_comp} ====================')\n",
    "    kf = KFold(n_splits=4, shuffle=True, random_state=42)\n",
    "    iou_scores = []\n",
    "    \n",
    "    for train_index, test_index in tqdm.tqdm(kf.split(train_indices)):\n",
    "        y_train = np.array([Y_enc[ind] for ind in train_index], dtype=object)\n",
    "        y_test = np.array([Y_enc[ind] for ind in test_index], dtype=object)\n",
    "\n",
    "        _, X_train, X_test = concatenate_cca_with_original(X_a, X_v, X_t, train_index, test_index, n_components=n_comp, algo=algo)\n",
    "        \n",
    "        # KNN classifier for multilabel classification\n",
    "        knn = KNeighborsClassifier(n_neighbors=K, p=1, weights='distance')\n",
    "        classifier = MultiLabelLocalClassifierPerParentNode(local_classifier=knn, n_jobs=-1)\n",
    "        classifier.fit(X_train, y_train)\n",
    "    \n",
    "        y_pred = classifier.predict(X_test)\n",
    "    \n",
    "        # Calculate IoU for this fold\n",
    "        fold_iou_score = calc_iou_from_hierarchy(y_test, y_pred)\n",
    "        iou_scores.append(fold_iou_score)\n",
    "\n",
    "    _, X, data_test = concatenate_cca_with_original(X_a, X_v, X_t, train_indices, test_indices, n_components=n_comp, algo=algo)\n",
    "    \n",
    "    knn = KNeighborsClassifier(n_neighbors=K, p=1, weights='distance')\n",
    "    classifier = MultiLabelLocalClassifierPerParentNode(local_classifier=knn, n_jobs=-1)\n",
    "    classifier.fit(X, np.array(Y_enc, dtype='object'))\n",
    "    Y_pred_test = classifier.predict(data_test)\n",
    "    test_iou = calc_iou_from_hierarchy(Y_test, Y_pred_test)\n",
    "    \n",
    "    average_iou = np.mean(iou_scores)\n",
    "    print(f\"iou: {average_iou:.4f}, test_iou: {test_iou:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d4e0c4b8-f0d5-40d6-a633-83b0c27dffa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================== algo = pls, n_comp = 32 ====================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63051685fe4b45158195a24e9e53dfe5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iou: 0.1404, test_iou: 0.6665\n",
      "==================== algo = pls, n_comp = 48 ====================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89696f426e4a4e9db429904b331bd650",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iou: 0.1401, test_iou: 0.6683\n",
      "==================== algo = pls, n_comp = 64 ====================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1dc4bc264bf3466e8b714e95334d9ff9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iou: 0.1463, test_iou: 0.6729\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(SEED)\n",
    "test_ratio = 0.2\n",
    "sample_size = X_a.shape[0]\n",
    "test_indices = np.random.choice(sample_size, int(test_ratio*sample_size), replace=False)\n",
    "train_indices = np.array(list(set(np.arange(sample_size)) - set(test_indices)))\n",
    "\n",
    "Y_enc = list(map(convert_ids_to_hierarchy, Y_ttl[train_indices]))\n",
    "Y_test = np.array(list(map(convert_ids_to_hierarchy, Y_ttl[test_indices])), dtype='object')\n",
    "\n",
    "K = 1\n",
    "algo = 'pls'\n",
    "\n",
    "for n_comp in [32, 48, 64]:\n",
    "    print(f'==================== algo = {algo}, n_comp = {n_comp} ====================')\n",
    "    kf = KFold(n_splits=4, shuffle=True, random_state=42)\n",
    "    iou_scores = []\n",
    "    \n",
    "    for train_index, test_index in tqdm.tqdm(kf.split(train_indices)):\n",
    "        y_train = np.array([Y_enc[ind] for ind in train_index], dtype=object)\n",
    "        y_test = np.array([Y_enc[ind] for ind in test_index], dtype=object)\n",
    "\n",
    "        _, X_train, X_test = concatenate_cca_with_original(X_a, X_t, X_v, train_index, test_index, n_components=n_comp, algo=algo)\n",
    "        \n",
    "        # KNN classifier for multilabel classification\n",
    "        knn = KNeighborsClassifier(n_neighbors=K, p=1, weights='distance')\n",
    "        classifier = MultiLabelLocalClassifierPerParentNode(local_classifier=knn, n_jobs=-1)\n",
    "        classifier.fit(X_train, y_train)\n",
    "    \n",
    "        y_pred = classifier.predict(X_test)\n",
    "    \n",
    "        # Calculate IoU for this fold\n",
    "        fold_iou_score = calc_iou_from_hierarchy(y_test, y_pred)\n",
    "        iou_scores.append(fold_iou_score)\n",
    "\n",
    "    _, X, data_test = concatenate_cca_with_original(X_a, X_t, X_v, train_indices, test_indices, n_components=n_comp, algo=algo)\n",
    "    \n",
    "    knn = KNeighborsClassifier(n_neighbors=K, p=1, weights='distance')\n",
    "    classifier = MultiLabelLocalClassifierPerParentNode(local_classifier=knn, n_jobs=-1)\n",
    "    classifier.fit(X, np.array(Y_enc, dtype='object'))\n",
    "    Y_pred_test = classifier.predict(data_test)\n",
    "    test_iou = calc_iou_from_hierarchy(Y_test, Y_pred_test)\n",
    "    \n",
    "    average_iou = np.mean(iou_scores)\n",
    "    print(f\"iou: {average_iou:.4f}, test_iou: {test_iou:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9be262ac-53ff-45c4-a880-f47168a0fc51",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### 1 CCA with original features and cross-product embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "15d2fa57-b0a0-41a7-b439-7e3cb11c10cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat_cca_and_cross_prod_emb(X_a, X_v, X_t, embeddings, train_indices, test_indices, n_components=64, normalize=True, algo='pls'):\n",
    "    \"\"\"\n",
    "    Apply CCA between each pair of domains (audio, video, text), concatenate the CCA features \n",
    "    and the original input features.\n",
    "    \n",
    "    Args:\n",
    "    X_a: Audio data\n",
    "    X_v: Video data\n",
    "    X_t: Text data\n",
    "    n_components: Number of CCA components to use\n",
    "    normalize: Whether to normalize the data\n",
    "    \n",
    "    Returns:\n",
    "    Concatenated latent embeddings from CCA on all pairs plus original features.\n",
    "    \"\"\"\n",
    "    # Run CCA on each pair of domains\n",
    "    pls_av, train_a_v, test_a_v = run_cca(X_a, X_v, train_indices, test_indices, n_components, normalize, algo=algo)\n",
    "    train_t, text_t = X_t[train_indices], X_t[test_indices]\n",
    "    \n",
    "    # Concatenate CCA embeddings from each pair of domains\n",
    "    cca_train = np.hstack([train_a_v, train_t, embeddings[train_indices]])\n",
    "    cca_test = np.hstack([test_a_v, text_t, embeddings[test_indices]])\n",
    "    \n",
    "    return pls_av, cca_train, cca_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b6200f50-9fe2-4286-b046-b95ac96884e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = compute_embedding_pytorch_chunked(X_a, X_v, X_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4d91999-83ce-43eb-9fa8-bc97227779c3",
   "metadata": {},
   "source": [
    "knn, base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "95120d74-c0a4-435f-a9da-54c5c0a8bd41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================== algo = pls, n_comp = 32 ====================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "106a8785cf3e40ae92bbb3bf647c066e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_iou: 0.1509, test_iou: 0.3655\n",
      "==================== algo = pls, n_comp = 48 ====================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28f84574114a435abdc2cdaa6283e685",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_iou: 0.1474, test_iou: 0.3643\n",
      "==================== algo = pls, n_comp = 64 ====================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5edf71a4500d484686e51619a45df29e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_iou: 0.1463, test_iou: 0.3502\n"
     ]
    }
   ],
   "source": [
    "K = 3\n",
    "\n",
    "np.random.seed(SEED)\n",
    "test_ratio = 0.2\n",
    "sample_size = X_a.shape[0]\n",
    "test_indices = np.random.choice(sample_size, int(test_ratio*sample_size), replace=False)\n",
    "train_indices = np.array(list(set(np.arange(sample_size)) - set(test_indices)))\n",
    "\n",
    "mlb = MultiLabelBinarizer()\n",
    "mlb.fit(Y_ttl)\n",
    "Y_enc = mlb.transform(Y_ttl[train_indices])\n",
    "Y_test = Y_ttl[test_indices]\n",
    "\n",
    "algo = 'pls'\n",
    "\n",
    "for n_comp in [32, 48, 64]:\n",
    "    print(f'==================== algo = {algo}, n_comp = {n_comp} ====================')\n",
    "    kf = KFold(n_splits=4, shuffle=True, random_state=42)\n",
    "    iou_scores = []\n",
    "    \n",
    "    for train_index, test_index in tqdm.tqdm(kf.split(train_indices)):\n",
    "        # Split the data into training and test sets for this fold\n",
    "        y_train, y_test = Y_enc[train_index], Y_enc[test_index]\n",
    "\n",
    "        _, X_train, X_test = concat_cca_and_cross_prod_emb(X_a, X_v, X_t, embeddings, train_index, test_index, n_components=n_comp, algo=algo)\n",
    "        \n",
    "        # KNN classifier for multilabel classification\n",
    "        knn = KNeighborsClassifier(n_neighbors=K, p=1, weights='distance')\n",
    "        multi_knn = MultiOutputClassifier(knn, n_jobs=-1)\n",
    "        multi_knn.fit(X_train, y_train)\n",
    "    \n",
    "        # Decode the predictions back to original multilabel format\n",
    "        y_pred = mlb.inverse_transform(multi_knn.predict(X_test))\n",
    "        y_test_decoded = mlb.inverse_transform(y_test)\n",
    "    \n",
    "        # Calculate IoU for this fold\n",
    "        fold_iou_score = calc_iou_from_ids(y_test_decoded, y_pred, labels_to_tags)\n",
    "        iou_scores.append(fold_iou_score)\n",
    "\n",
    "    _, X, data_test = concat_cca_and_cross_prod_emb(X_a, X_v, X_t, embeddings, train_indices, test_indices, n_components=n_comp, algo=algo)\n",
    "    \n",
    "    knn = KNeighborsClassifier(n_neighbors=K, p=1, weights='distance')\n",
    "    multi_knn = MultiOutputClassifier(knn, n_jobs=-1)\n",
    "    multi_knn.fit(X, Y_enc)\n",
    "    Y_pred_test = mlb.inverse_transform(multi_knn.predict(data_test))\n",
    "    test_iou = calc_iou_from_ids(Y_test, Y_pred_test, labels_to_tags)\n",
    "\n",
    "    average_iou = np.mean(iou_scores)\n",
    "    print(f\"val_iou: {average_iou:.4f}, test_iou: {test_iou:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3adee24-a53e-47df-8665-8b431ed2e067",
   "metadata": {},
   "source": [
    "knn, hierarchy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c14a4deb-f36c-416b-b246-4bc803dace6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================== algo = pls, n_comp = 32 ====================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "106f48c2bd2745c7a54f37cfbfdb395e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iou: 0.1521, test_iou: 0.3688\n",
      "==================== algo = pls, n_comp = 48 ====================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc1e9d9cc1b84b9aa127fbacea188d8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iou: 0.1490, test_iou: 0.3729\n",
      "==================== algo = pls, n_comp = 64 ====================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69b3ac21cae147e8a87a1c3facef0f05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iou: 0.1468, test_iou: 0.3773\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(SEED)\n",
    "test_ratio = 0.2\n",
    "sample_size = X_a.shape[0]\n",
    "test_indices = np.random.choice(sample_size, int(test_ratio*sample_size), replace=False)\n",
    "train_indices = np.array(list(set(np.arange(sample_size)) - set(test_indices)))\n",
    "\n",
    "Y_enc = list(map(convert_ids_to_hierarchy, Y_ttl[train_indices]))\n",
    "Y_test = np.array(list(map(convert_ids_to_hierarchy, Y_ttl[test_indices])), dtype='object')\n",
    "\n",
    "K = 1\n",
    "algo = 'pls'\n",
    "\n",
    "for n_comp in [32, 48, 64]:\n",
    "    print(f'==================== algo = {algo}, n_comp = {n_comp} ====================')\n",
    "    kf = KFold(n_splits=4, shuffle=True, random_state=42)\n",
    "    iou_scores = []\n",
    "    \n",
    "    for train_index, test_index in tqdm.tqdm(kf.split(train_indices)):\n",
    "        y_train = np.array([Y_enc[ind] for ind in train_index], dtype=object)\n",
    "        y_test = np.array([Y_enc[ind] for ind in test_index], dtype=object)\n",
    "\n",
    "        _, X_train, X_test = concat_cca_and_cross_prod_emb(X_a, X_v, X_t, embeddings, train_index, test_index, n_components=n_comp, algo=algo)\n",
    "        \n",
    "        # KNN classifier for multilabel classification\n",
    "        knn = KNeighborsClassifier(n_neighbors=K, p=1, weights='distance')\n",
    "        classifier = MultiLabelLocalClassifierPerParentNode(local_classifier=knn, n_jobs=-1)\n",
    "        classifier.fit(X_train, y_train)\n",
    "    \n",
    "        y_pred = classifier.predict(X_test)\n",
    "    \n",
    "        # Calculate IoU for this fold\n",
    "        fold_iou_score = calc_iou_from_hierarchy(y_test, y_pred)\n",
    "        iou_scores.append(fold_iou_score)\n",
    "\n",
    "    _, X, data_test = concat_cca_and_cross_prod_emb(X_a, X_v, X_t, embeddings, train_indices, test_indices, n_components=n_comp, algo=algo)\n",
    "    \n",
    "    knn = KNeighborsClassifier(n_neighbors=K, p=1, weights='distance')\n",
    "    classifier = MultiLabelLocalClassifierPerParentNode(local_classifier=knn, n_jobs=-1)\n",
    "    classifier.fit(X, np.array(Y_enc, dtype='object'))\n",
    "    Y_pred_test = classifier.predict(data_test)\n",
    "    test_iou = calc_iou_from_hierarchy(Y_test, Y_pred_test)\n",
    "    \n",
    "    average_iou = np.mean(iou_scores)\n",
    "    print(f\"iou: {average_iou:.4f}, test_iou: {test_iou:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f520c063-9adb-43c0-a2a6-8782af7fcae8",
   "metadata": {},
   "source": [
    "## Final solution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43303a29-fa27-4ee5-95ae-a7ba26b0c3a1",
   "metadata": {},
   "source": [
    "0. 1-NN ONLY on video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40570429-2706-4c7c-a182-198ccc21bb51",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "61ad49f8-5b93-45a0-94dd-590104836aa6",
   "metadata": {},
   "source": [
    "1. Concatenation of 3 PLS, kNN naive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "de85a218-49cd-4ccd-9f54-42739ffab78e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# K = 3\n",
    "\n",
    "# np.random.seed(SEED)\n",
    "# test_ratio = 0.2\n",
    "# sample_size = embeddings.shape[0]\n",
    "# test_indices = np.random.choice(sample_size, int(test_ratio*sample_size), replace=False)\n",
    "# train_indices = np.array(list(set(np.arange(sample_size)) - set(test_indices)))\n",
    "\n",
    "# mlb = MultiLabelBinarizer()\n",
    "# mlb.fit(Y_ttl)\n",
    "# Y_enc = mlb.transform(Y_ttl[train_indices])\n",
    "# Y_test = Y_ttl[test_indices]\n",
    "# algo = 'pls'\n",
    "\n",
    "# for n_comp in [32, 48, 64]:\n",
    "#     print(f'==================== algo = {algo}, n_comp = {n_comp} ====================')\n",
    "#     kf = KFold(n_splits=4, shuffle=True, random_state=42)\n",
    "#     iou_scores = []\n",
    "    \n",
    "#     for train_index, test_index in tqdm.tqdm(kf.split(train_indices)):\n",
    "#         # Split the data into training and test sets for this fold\n",
    "#         y_train, y_test = Y_enc[train_index], Y_enc[test_index]\n",
    "\n",
    "#         _, X_train, X_test = concatenate_cca_embeddings(X_a, X_v, X_t, train_index, test_index, n_components=n_comp, algo=algo)\n",
    "        \n",
    "#         # KNN classifier for multilabel classification\n",
    "#         knn = KNeighborsClassifier(n_neighbors=K, p=1, weights='distance')\n",
    "#         multi_knn = MultiOutputClassifier(knn, n_jobs=-1)\n",
    "#         multi_knn.fit(X_train, y_train)\n",
    "    \n",
    "#         # Decode the predictions back to original multilabel format\n",
    "#         y_pred = mlb.inverse_transform(multi_knn.predict(X_test))\n",
    "#         y_test_decoded = mlb.inverse_transform(y_test)\n",
    "    \n",
    "#         # Calculate IoU for this fold\n",
    "#         fold_iou_score = calc_iou_from_ids(y_test_decoded, y_pred, labels_to_tags)\n",
    "#         iou_scores.append(fold_iou_score)\n",
    "\n",
    "#     _, X, data_test = concatenate_cca_embeddings(X_a, X_v, X_t, train_indices, test_indices, n_components=n_comp, algo=algo)\n",
    "    \n",
    "#     knn = KNeighborsClassifier(n_neighbors=K, p=1, weights='distance')\n",
    "#     multi_knn = MultiOutputClassifier(knn, n_jobs=-1)\n",
    "#     multi_knn.fit(X, Y_enc)\n",
    "#     Y_pred_test = mlb.inverse_transform(multi_knn.predict(data_test))\n",
    "#     test_iou = calc_iou_from_ids(Y_test, Y_pred_test, labels_to_tags)\n",
    "\n",
    "#     average_iou = np.mean(iou_scores)\n",
    "#     print(f\"iou: {average_iou:.4f}, test_iou: {test_iou:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "972bdb3d-c7d8-43ab-b98e-c55fa39060aa",
   "metadata": {},
   "source": [
    "2. 1 PLS with original features, kNN naive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45076809-563b-4054-af0c-672a631effa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# K = 3\n",
    "\n",
    "# np.random.seed(SEED)\n",
    "# test_ratio = 0.2\n",
    "# sample_size = X_a.shape[0]\n",
    "# test_indices = np.random.choice(sample_size, int(test_ratio*sample_size), replace=False)\n",
    "# train_indices = np.array(list(set(np.arange(sample_size)) - set(test_indices)))\n",
    "\n",
    "# mlb = MultiLabelBinarizer()\n",
    "# mlb.fit(Y_ttl)\n",
    "# Y_enc = mlb.transform(Y_ttl[train_indices])\n",
    "# Y_test = Y_ttl[test_indices]\n",
    "\n",
    "# algo = 'pls'\n",
    "\n",
    "# for n_comp in [32, 48, 64]:\n",
    "#     print(f'==================== algo = {algo}, n_comp = {n_comp} ====================')\n",
    "#     kf = KFold(n_splits=4, shuffle=True, random_state=42)\n",
    "#     iou_scores = []\n",
    "    \n",
    "#     for train_index, test_index in tqdm.tqdm(kf.split(train_indices)):\n",
    "#         # Split the data into training and test sets for this fold\n",
    "#         y_train, y_test = Y_enc[train_index], Y_enc[test_index]\n",
    "\n",
    "#         _, X_train, X_test = concatenate_cca_with_original(X_a, X_v, X_t, train_index, test_index, n_components=n_comp, algo=algo)\n",
    "        \n",
    "#         # KNN classifier for multilabel classification\n",
    "#         knn = KNeighborsClassifier(n_neighbors=K, p=1, weights='distance')\n",
    "#         multi_knn = MultiOutputClassifier(knn, n_jobs=-1)\n",
    "#         multi_knn.fit(X_train, y_train)\n",
    "    \n",
    "#         # Decode the predictions back to original multilabel format\n",
    "#         y_pred = mlb.inverse_transform(multi_knn.predict(X_test))\n",
    "#         y_test_decoded = mlb.inverse_transform(y_test)\n",
    "    \n",
    "#         # Calculate IoU for this fold\n",
    "#         fold_iou_score = calc_iou_from_ids(y_test_decoded, y_pred, labels_to_tags)\n",
    "#         iou_scores.append(fold_iou_score)\n",
    "\n",
    "#     _, X, data_test = concatenate_cca_with_original(X_a, X_v, X_t, train_indices, test_indices, n_components=n_comp, algo=algo)\n",
    "    \n",
    "#     knn = KNeighborsClassifier(n_neighbors=K, p=1, weights='distance')\n",
    "#     multi_knn = MultiOutputClassifier(knn, n_jobs=-1)\n",
    "#     multi_knn.fit(X, Y_enc)\n",
    "#     Y_pred_test = mlb.inverse_transform(multi_knn.predict(data_test))\n",
    "#     test_iou = calc_iou_from_ids(Y_test, Y_pred_test, labels_to_tags)\n",
    "\n",
    "#     average_iou = np.mean(iou_scores)\n",
    "#     print(f\"val_iou: {average_iou:.4f}, test_iou: {test_iou:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (video-tag)",
   "language": "python",
   "name": "video-tag"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
